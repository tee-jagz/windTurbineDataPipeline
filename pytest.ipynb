{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import mean, stddev, col, abs, date_format, min, max, lag, lead, coalesce, avg, to_date, to_timestamp\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pyspark.sql import SparkSession, dataframe\n",
    "import pyspark.pandas as ps\n",
    "\n",
    "import psycopg2\n",
    "from psycopg2.extras import execute_values\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "\n",
    "DB_NAME = os.getenv(\"DB_NAME\")\n",
    "DB_USER = os.getenv(\"DB_USER\")\n",
    "DB_PASSWORD = os.getenv(\"DB_PASSWORD\")\n",
    "DB_HOST = os.getenv(\"DB_HOST\")\n",
    "\n",
    "db_params = {\n",
    "    'dbname': DB_NAME,\n",
    "    'user': DB_USER,\n",
    "    'password': DB_PASSWORD,\n",
    "    'host': DB_HOST\n",
    "}\n",
    "\n",
    "conn = psycopg2.connect(**db_params)\n",
    "conn.autocommit = True\n",
    "cursor = conn.cursor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark Session initialized\n"
     ]
    }
   ],
   "source": [
    "# Initialize Spark Session\n",
    "spark = SparkSession.builder.master('local').appName(\"WindTurbineDataPipeline\") \\\n",
    ".getOrCreate()\n",
    "\n",
    "print('Spark Session initialized')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_last_times() -> dict:\n",
    "    cursor.execute('SELECT turbine_id, MAX(timestamp) FROM turbine_data_raw GROUP BY turbine_id')\n",
    "    # cursor.execute('SELECT turbine_id, timestamp FROM turbine_data_raw')\n",
    "    times = {turbineid:timestamp for turbineid, timestamp in cursor.fetchall()}\n",
    "    return times\n",
    "\n",
    "\n",
    "# Calculate the mean of the nearest non-null values\n",
    "'''\n",
    "Reason: The data is sampled every 1 hour, so the nearest non-null values are likely \n",
    "to be similar and it is good to keep as many data points as possible for analysis\n",
    "'''\n",
    "def fill_nas_with_mean(df: dataframe.DataFrame, colName: str) -> dataframe.DataFrame:\n",
    "    windowSpec = Window.partitionBy('turbine_id').orderBy('timestamp')\n",
    "    df = df.withColumn('prev_value', lag(colName, 1).over(windowSpec))\n",
    "    df = df.withColumn('next_value', lead(colName, 1).over(windowSpec))\n",
    "    df = df.withColumn('mean_nearest', (col('prev_value') + col('next_value'))/2)\n",
    "    df = df.withColumn(colName, coalesce(col(colName), col('mean_nearest')))\n",
    "    df = df.drop('prev_value', 'next_value', 'mean_nearest')\n",
    "    return df\n",
    "\n",
    "\n",
    "# Replace outliers with the mean of the nearest non-null values\n",
    "def replace_outliers_with_mean(df: dataframe.DataFrame, colName: str) -> dataframe.DataFrame:\n",
    "    windowSpec = Window.partitionBy('turbine_id').orderBy('timestamp')\n",
    "    df = df.withColumn('mean_nearest', mean(col(colName)).over(windowSpec))\n",
    "    df = df.withColumn(colName, coalesce(col(colName), col('mean_nearest')))\n",
    "    df = df.drop('mean_nearest')\n",
    "    return df\n",
    "\n",
    "\n",
    "# Calculate the mean, min and max values of each turbine for each day\n",
    "def calculate_daily_stats(df: dataframe.DataFrame, colName: str) -> dataframe.DataFrame:\n",
    "    df = df.groupBy('turbine_id', 'date').agg(\n",
    "        mean(col(colName)).alias('avg_'+colName),\n",
    "        min(col(colName)).alias('min_'+colName),\n",
    "        max(col(colName)).alias('max_'+colName)\n",
    "    )\n",
    "    return df\n",
    "\n",
    "\n",
    "# Detect anomalies using the 2-sigma rule\n",
    "def detect_anomalies(df: dataframe.DataFrame, colName: str) -> dataframe.DataFrame:\n",
    "    windowSpec = Window.partitionBy('time', 'turbine_id')\n",
    "    df = df.withColumn('mean_'+colName, mean(col(colName)).over(windowSpec)) \\\n",
    "        .withColumn('stddev_'+colName, stddev(col(colName)).over(windowSpec))\n",
    "    df = df.withColumn('lower_bound_'+colName, col('mean_'+colName) - 2 * col('stddev_'+colName)) \\\n",
    "        .withColumn('upper_bound_'+colName, col('mean_'+colName) + 2 * col('stddev_'+colName))\n",
    "    df = df.filter((col(colName) < col('lower_bound_'+colName)) | (col(colName) > col('upper_bound_'+colName)))\n",
    "    df = df.drop('mean_'+colName, 'stddev_'+colName, 'wind-speed', 'wind-direction')\n",
    "    return df\n",
    "\n",
    "\n",
    "# Filter out the data that has already been uploaded\n",
    "def filter_for_new_data(df: dataframe.DataFrame, last_upload_times: dict) -> dataframe.DataFrame:\n",
    "    # Convert last upload times to a PySpark DataFrame\n",
    "    last_upload_df = spark.createDataFrame(last_upload_times.items(), ['turbine_id2', 'last_timestamp'])\n",
    "    \n",
    "    # Join the new data with the last upload timestamps to filter out old rows\n",
    "    df_filtered = df.join(last_upload_df, ((df['turbine_id'] == last_upload_df['turbine_id2']) & (df['timestamp'] == last_upload_df['last_timestamp'])), 'left')\n",
    "           # .filter((df['timestamp'] > last_upload_df['last_timestamp']) | (last_upload_df['last_timestamp'].isNull()))\n",
    "    df_filtered = df_filtered.filter(df_filtered['last_timestamp'].isNull())\n",
    "    df_filtered = df_filtered.sort('timestamp')\n",
    "\n",
    "    #print(df_filtered.show())\n",
    "        \n",
    "    \n",
    "    # Drop the last_timestamp column\n",
    "    df_filtered = df_filtered.drop('last_timestamp', 'turbine_id2')\n",
    "    return df_filtered\n",
    "\n",
    "\n",
    "def upload_data_to_sql(df: dataframe.DataFrame, table_name: str) -> None:\n",
    "    df.write \\\n",
    "        .format(\"jdbc\") \\\n",
    "        .option(\"url\", f\"jdbc:postgresql://{DB_HOST}/{DB_NAME}\") \\\n",
    "        .option(\"dbtable\", table_name) \\\n",
    "        .option(\"user\", DB_USER) \\\n",
    "        .option(\"password\", DB_PASSWORD) \\\n",
    "        .option(\"driver\", \"org.postgresql.Driver\") \\\n",
    "        .save(mode='append')\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV files read into DataFrames\n",
      "DataFrames joined\n",
      "Null values filled\n",
      "Outliers replaced\n",
      "Date and time columns added\n",
      "Anomalies detected\n"
     ]
    }
   ],
   "source": [
    "# Read the CSV files into a DataFrames\n",
    "sdf1 = spark.read.csv('./raw_data1/data_group_1.csv', header=True, inferSchema=True)\n",
    "sdf2 = spark.read.csv('./raw_data1/data_group_2.csv', header=True, inferSchema=True)\n",
    "sdf3 = spark.read.csv('./raw_data1/data_group_3.csv', header=True, inferSchema=True)\n",
    "print('CSV files read into DataFrames')\n",
    "\n",
    "# Join the DataFrames\n",
    "sdf = sdf1.union(sdf2).union(sdf3)\n",
    "print('DataFrames joined')\n",
    "\n",
    "# Check if there are any null values and fill them with the mean of the nearest non-null values\n",
    "clean_df = fill_nas_with_mean(sdf, 'wind_speed')\n",
    "print('Null values filled')\n",
    "\n",
    "# Check outliers and fill them with the mean of the nearest non-null values\n",
    "clean_df = replace_outliers_with_mean(clean_df, 'wind_speed')\n",
    "print('Outliers replaced')\n",
    "\n",
    "# Add a column for the date and time\n",
    "sdf = sdf.withColumn('date', (col('timestamp')).cast('date'))\n",
    "sdf = sdf.withColumn('time', to_timestamp('timestamp', 'HH:mm:ss'))\n",
    "clean_df = clean_df.withColumn('date', (col('timestamp')).cast('date'))\n",
    "clean_df = clean_df.withColumn('time', to_timestamp('timestamp', 'HH:mm:ss'))\n",
    "print('Date and time columns added')\n",
    "\n",
    "# Get the last uploaded times for each turbine\n",
    "last_uploaded_dates = get_last_times()\n",
    "\n",
    "# Calculate the anomalies for each turbine\n",
    "anomalies_df = detect_anomalies(clean_df, 'power_output')\n",
    "print('Anomalies detected')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{4: datetime.datetime(2022, 3, 25, 23, 0),\n",
       " 14: datetime.datetime(2022, 3, 19, 23, 0),\n",
       " 3: datetime.datetime(2022, 3, 25, 23, 0),\n",
       " 10: datetime.datetime(2022, 3, 19, 23, 0),\n",
       " 9: datetime.datetime(2022, 3, 19, 23, 0),\n",
       " 7: datetime.datetime(2022, 3, 19, 23, 0),\n",
       " 13: datetime.datetime(2022, 3, 19, 23, 0),\n",
       " 1: datetime.datetime(2022, 3, 25, 23, 0),\n",
       " 5: datetime.datetime(2022, 3, 25, 23, 0),\n",
       " 2: datetime.datetime(2022, 3, 25, 23, 0),\n",
       " 15: datetime.datetime(2022, 3, 19, 23, 0),\n",
       " 6: datetime.datetime(2022, 3, 19, 23, 0),\n",
       " 12: datetime.datetime(2022, 3, 19, 23, 0),\n",
       " 8: datetime.datetime(2022, 3, 19, 23, 0),\n",
       " 11: datetime.datetime(2022, 3, 19, 23, 0)}"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_last_times()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o1090.showString.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 11.0 failed 1 times, most recent failure: Lost task 0.0 in stage 11.0 (TID 17) (10.164.207.254 executor driver): org.apache.spark.SparkException: Python worker failed to connect back.\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:203)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:109)\r\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:124)\r\n\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:174)\r\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:67)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\r\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\r\n\tat java.base/java.lang.Thread.run(Thread.java:1583)\r\nCaused by: java.net.SocketTimeoutException: Accept timed out\r\n\tat java.base/sun.nio.ch.NioSocketImpl.timedAccept(NioSocketImpl.java:701)\r\n\tat java.base/sun.nio.ch.NioSocketImpl.accept(NioSocketImpl.java:745)\r\n\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:698)\r\n\tat java.base/java.net.ServerSocket.platformImplAccept(ServerSocket.java:663)\r\n\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:639)\r\n\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:585)\r\n\tat java.base/java.net.ServerSocket.accept(ServerSocket.java:543)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:190)\r\n\t... 34 more\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2844)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2780)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2779)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2779)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1242)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1242)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1242)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3048)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2982)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2971)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\nCaused by: org.apache.spark.SparkException: Python worker failed to connect back.\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:203)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:109)\r\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:124)\r\n\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:174)\r\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:67)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\r\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\r\n\tat java.base/java.lang.Thread.run(Thread.java:1583)\r\nCaused by: java.net.SocketTimeoutException: Accept timed out\r\n\tat java.base/sun.nio.ch.NioSocketImpl.timedAccept(NioSocketImpl.java:701)\r\n\tat java.base/sun.nio.ch.NioSocketImpl.accept(NioSocketImpl.java:745)\r\n\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:698)\r\n\tat java.base/java.net.ServerSocket.platformImplAccept(ServerSocket.java:663)\r\n\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:639)\r\n\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:585)\r\n\tat java.base/java.net.ServerSocket.accept(ServerSocket.java:543)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:190)\r\n\t... 34 more\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[56], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m filtered_raw_df \u001b[38;5;241m=\u001b[39m filter_for_new_data(sdf, last_uploaded_dates)\n\u001b[1;32m----> 2\u001b[0m \u001b[43mfiltered_raw_df\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshow\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\project\\pyspark\\venv\\lib\\site-packages\\pyspark\\sql\\dataframe.py:959\u001b[0m, in \u001b[0;36mDataFrame.show\u001b[1;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[0;32m    953\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m PySparkTypeError(\n\u001b[0;32m    954\u001b[0m         error_class\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNOT_BOOL\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    955\u001b[0m         message_parameters\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marg_name\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvertical\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marg_type\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mtype\u001b[39m(vertical)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m},\n\u001b[0;32m    956\u001b[0m     )\n\u001b[0;32m    958\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(truncate, \u001b[38;5;28mbool\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m truncate:\n\u001b[1;32m--> 959\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshowString\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvertical\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m    960\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    961\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\project\\pyspark\\venv\\lib\\site-packages\\py4j\\java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[1;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[0;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[1;32mc:\\project\\pyspark\\venv\\lib\\site-packages\\pyspark\\errors\\exceptions\\captured.py:179\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    177\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m    178\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 179\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m f(\u001b[38;5;241m*\u001b[39ma, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw)\n\u001b[0;32m    180\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    181\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[1;32mc:\\project\\pyspark\\venv\\lib\\site-packages\\py4j\\protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[0;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[1;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[0;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[0;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[0;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling o1090.showString.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 11.0 failed 1 times, most recent failure: Lost task 0.0 in stage 11.0 (TID 17) (10.164.207.254 executor driver): org.apache.spark.SparkException: Python worker failed to connect back.\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:203)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:109)\r\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:124)\r\n\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:174)\r\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:67)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\r\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\r\n\tat java.base/java.lang.Thread.run(Thread.java:1583)\r\nCaused by: java.net.SocketTimeoutException: Accept timed out\r\n\tat java.base/sun.nio.ch.NioSocketImpl.timedAccept(NioSocketImpl.java:701)\r\n\tat java.base/sun.nio.ch.NioSocketImpl.accept(NioSocketImpl.java:745)\r\n\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:698)\r\n\tat java.base/java.net.ServerSocket.platformImplAccept(ServerSocket.java:663)\r\n\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:639)\r\n\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:585)\r\n\tat java.base/java.net.ServerSocket.accept(ServerSocket.java:543)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:190)\r\n\t... 34 more\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2844)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2780)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2779)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2779)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1242)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1242)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1242)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3048)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2982)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2971)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\nCaused by: org.apache.spark.SparkException: Python worker failed to connect back.\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:203)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:109)\r\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:124)\r\n\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:174)\r\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:67)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\r\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\r\n\tat java.base/java.lang.Thread.run(Thread.java:1583)\r\nCaused by: java.net.SocketTimeoutException: Accept timed out\r\n\tat java.base/sun.nio.ch.NioSocketImpl.timedAccept(NioSocketImpl.java:701)\r\n\tat java.base/sun.nio.ch.NioSocketImpl.accept(NioSocketImpl.java:745)\r\n\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:698)\r\n\tat java.base/java.net.ServerSocket.platformImplAccept(ServerSocket.java:663)\r\n\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:639)\r\n\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:585)\r\n\tat java.base/java.net.ServerSocket.accept(ServerSocket.java:543)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:190)\r\n\t... 34 more\r\n"
     ]
    }
   ],
   "source": [
    "filtered_raw_df = filter_for_new_data(sdf, last_uploaded_dates)\n",
    "filtered_raw_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter out the data that has already been uploaded\n",
    "filtered_raw_df = filter_for_new_data(sdf, last_uploaded_dates)\n",
    "filtered_clean_df = filter_for_new_data(clean_df, last_uploaded_dates)\n",
    "filtered_anomalies_df = filter_for_new_data(anomalies_df, last_uploaded_dates)\n",
    "filtered_anomalies_df = filtered_anomalies_df.drop('wind_speed', 'wind_direction')\n",
    "\n",
    "print(filtered_anomalies_df.show())\n",
    "print('New data filtered')\n",
    "\n",
    "\n",
    "# Upload the raw and processed dataframes to the database\n",
    "upload_data_to_sql(filtered_raw_df, 'turbine_data_raw')\n",
    "upload_data_to_sql(filtered_clean_df, 'turbine_data_cleaned')\n",
    "print('Data uploaded')\n",
    "\n",
    "# Calculate the daily statistics for each turbine\n",
    "daily_stats_df = calculate_daily_stats(clean_df, 'power_output')\n",
    "print('Daily statistics calculated')\n",
    "\n",
    "# Upload the daily statistics and anomalies to the database\n",
    "upload_data_to_sql(daily_stats_df, 'summary_statistics')\n",
    "print('Daily statistics uploaded')\n",
    "\n",
    "# Upload the anomalies to the database\n",
    "upload_data_to_sql(filtered_anomalies_df, 'anomalies')\n",
    "print('Anomalies uploaded')\n",
    "\n",
    "# Close the connection and cursor\n",
    "cursor.close()\n",
    "conn.close()\n",
    "\n",
    "# Stop Spark Session\n",
    "spark.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Spark Session\n",
    "spark = SparkSession.builder.appName(\"WindTurbineDataPipeline\").getOrCreate()\n",
    "\n",
    "# Read the CSV file into a DataFrame\n",
    "sdf1 = spark.read.csv('./raw_data/data_group_1.csv', header=True, inferSchema=True)\n",
    "sdf2 = spark.read.csv('./raw_data/data_group_2.csv', header=True, inferSchema=True)\n",
    "sdf3 = spark.read.csv('./raw_data/data_group_3.csv', header=True, inferSchema=True)\n",
    "raw_sdf = sdf1.union(sdf2).union(sdf3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+----------+----------+--------------+------------+\n",
      "|          timestamp|turbine_id|wind_speed|wind_direction|power_output|\n",
      "+-------------------+----------+----------+--------------+------------+\n",
      "|2022-03-01 00:00:00|         1|      11.8|           169|         2.7|\n",
      "|2022-03-01 00:00:00|         2|      11.6|            24|         2.2|\n",
      "|2022-03-01 00:00:00|         3|      13.8|           335|         2.3|\n",
      "|2022-03-01 00:00:00|         4|      12.8|           238|         1.9|\n",
      "|2022-03-01 00:00:00|         5|      11.4|           103|         3.5|\n",
      "|2022-03-01 01:00:00|         1|      11.6|           152|         4.4|\n",
      "|2022-03-01 01:00:00|         2|      12.8|            35|         4.2|\n",
      "|2022-03-01 01:00:00|         3|      10.4|           169|         1.9|\n",
      "|2022-03-01 01:00:00|         4|      13.9|           170|         2.4|\n",
      "|2022-03-01 01:00:00|         5|      12.1|           165|         4.0|\n",
      "|2022-03-01 02:00:00|         1|      13.8|            73|         2.9|\n",
      "|2022-03-01 02:00:00|         2|       9.9|           103|         3.8|\n",
      "|2022-03-01 02:00:00|         3|      12.2|           188|         4.0|\n",
      "|2022-03-01 02:00:00|         4|      13.2|            21|         4.1|\n",
      "|2022-03-01 02:00:00|         5|       9.4|            10|         2.7|\n",
      "|2022-03-01 03:00:00|         1|      10.5|            61|         1.8|\n",
      "|2022-03-01 03:00:00|         2|      12.4|           274|         3.4|\n",
      "|2022-03-01 03:00:00|         3|      14.3|           232|         2.2|\n",
      "|2022-03-01 03:00:00|         4|      11.5|            32|         3.3|\n",
      "|2022-03-01 03:00:00|         5|      11.7|           305|         2.2|\n",
      "+-------------------+----------+----------+--------------+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "raw_sdf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+----------+----------+--------------+------------+----------+--------+\n",
      "|          timestamp|turbine_id|wind_speed|wind_direction|power_output|      date|    time|\n",
      "+-------------------+----------+----------+--------------+------------+----------+--------+\n",
      "|2022-03-01 00:00:00|         1|      11.8|           169|         2.7|2022-03-01|00:00:00|\n",
      "|2022-03-01 00:00:00|         2|      11.6|            24|         2.2|2022-03-01|00:00:00|\n",
      "|2022-03-01 00:00:00|         3|      13.8|           335|         2.3|2022-03-01|00:00:00|\n",
      "|2022-03-01 00:00:00|         4|      12.8|           238|         1.9|2022-03-01|00:00:00|\n",
      "+-------------------+----------+----------+--------------+------------+----------+--------+\n",
      "only showing top 4 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "raw_sdf = raw_sdf.withColumn('date', (col('timestamp')).cast('date'))\n",
    "raw_sdf = raw_sdf.withColumn('time', date_format('timestamp', 'HH:mm:ss'))\n",
    "raw_sdf.show(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_last_times() -> dict:\n",
    "    cursor.execute('SELECT turbineid, MAX(timestamp) FROM turbine_data_raw GROUP BY turbineid')\n",
    "    dates = {a:b for a,b in cursor.fetchall()}\n",
    "    return dates\n",
    "\n",
    "# Calculate the mean of the nearest non-null values\n",
    "def fill_nas_with_mean(df: dataframe.DataFrame, colName: str) -> dataframe.DataFrame:\n",
    "    windowSpec = Window.partitionBy('turbine_id').orderBy('timestamp')\n",
    "    df = df.withColumn('prev_value', lag(colName, 1).over(windowSpec))\n",
    "    df = df.withColumn('next_value', lead(colName, 1).over(windowSpec))\n",
    "    df = df.withColumn('mean_nearest', (col('prev_value') + col('next_value'))/2)\n",
    "    df = df.withColumn(colName, coalesce(col(colName), col('mean_nearest')))\n",
    "    df = df.drop('prev_value', 'next_value', 'mean_nearest')\n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "def calculate_daily_stats(df: dataframe.DataFrame, colName: str) -> dataframe.DataFrame:\n",
    "    df = df.groupBy('turbine_id', 'date').agg(\n",
    "        mean(col(colName)).alias('avg_'+colName),\n",
    "        min(col(colName)).alias('min_'+colName),\n",
    "        max(col(colName)).alias('max_'+colName)\n",
    "    )\n",
    "    return df\n",
    "\n",
    "\n",
    "def detect_anomalies(df: dataframe.DataFrame, colName: str) -> dataframe.DataFrame:\n",
    "    windowSpec = Window.partitionBy('time', 'turbine_id')\n",
    "    df = df.withColumn('mean_'+colName, mean(col(colName)).over(windowSpec)) \\\n",
    "        .withColumn('stddev_'+colName, stddev(col(colName)).over(windowSpec))\n",
    "    df = df.withColumn('lower_bound_'+colName, col('mean_'+colName) - 2 * col('stddev_'+colName)) \\\n",
    "        .withColumn('upper_bound_'+colName, col('mean_'+colName) + 2 * col('stddev_'+colName))\n",
    "    \n",
    "    # TODO: MAKE IT FILTER FOR DATES THAT ARE NOT IN THE DATABASE\n",
    "    df = df.filter((col(colName) < col('lower_bound_'+colName)) | (col(colName) > col('upper_bound_'+colName)))\n",
    "    df = df.drop('mean_'+colName, 'stddev_'+colName)\n",
    "    return df\n",
    "\n",
    "\n",
    "def filter_for_new_data(df: dataframe.DataFrame, last_upload_times: dict) -> dataframe.DataFrame:\n",
    "    # Convert last upload times to a PySpark DataFrame\n",
    "    last_upload_df = spark.createDataFrame(last_upload_times.items(), ['TurbineID', 'LastTimestamp'])\n",
    "    \n",
    "    # Join the new data with the last upload timestamps to filter out old rows\n",
    "    df_filtered = df.join(last_upload_df, 'TurbineID', 'left_anti') \\\n",
    "        .filter((df['Timestamp'] > last_upload_df['LastTimestamp']) | (last_upload_df['LastTimestamp'].isNull()))\n",
    "    return df_filtered\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "# # Join the new data with the last upload timestamps to filter out old rows\n",
    "# df_filtered = df.join(last_upload_df, 'TurbineID', 'left_anti') \\\n",
    "#     .filter((df['Timestamp'] > last_upload_df['LastTimestamp']) | (last_upload_df['LastTimestamp'].isNull()))\n",
    "\n",
    "\n",
    "# # Add columns for mean and standard deviation of power output\n",
    "# df = df.withColumn('mean_power', mean('PowerOutput').over(windowSpec)) \\\n",
    "#        .withColumn('stddev_power', stddev('PowerOutput').over(windowSpec))\n",
    "\n",
    "# # Calculate the bounds for detecting anomalies\n",
    "# df = df.withColumn('lower_bound', col('mean_power') - 2 * col('stddev_power')) \\\n",
    "#        .withColumn('upper_bound', col('mean_power') + 2 * col('stddev_power'))\n",
    "\n",
    "# # Detect anomalies by checking if the power output is outside the bounds\n",
    "# df_anomalies = df.filter((col('PowerOutput') < col('lower_bound')) | (col('PowerOutput') > col('upper_bound')))\n",
    "\n",
    "# # Select only the relevant columns to display\n",
    "# df_anomalies = df_anomalies.select('Date', 'Time', 'TurbineID', 'PowerOutput', 'mean_power', 'stddev_power', 'lower_bound', 'upper_bound')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "____________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\n{'_'*60}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['timestamp', 'turbine_id', 'wind_speed', 'wind_direction', 'power_output']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize Spark Session\n",
    "spark = SparkSession.builder.appName(\"WindTurbineDataPipeline\").config('spark.jars', \"./jdbc_driver/postgresql-42.7.1.jar\").getOrCreate()\n",
    "\n",
    "tdf = spark.read.csv('./raw_data/test.csv', header=True, inferSchema=True)\n",
    "\n",
    "tdf.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+--------------------+----------------+----------------+\n",
      "|turbine_id|      date|average_power_output|min_power_output|max_power_output|\n",
      "+----------+----------+--------------------+----------------+----------------+\n",
      "|         3|2022-03-23|   2.795833333333333|             1.5|             4.2|\n",
      "|         3|2022-03-21|   3.341666666666667|             1.5|             4.4|\n",
      "|         3|2022-03-16|   2.904166666666667|             1.8|             4.3|\n",
      "|         5|2022-03-06|   3.008333333333334|             1.5|             4.4|\n",
      "+----------+----------+--------------------+----------------+----------------+\n",
      "only showing top 4 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "calculate_daily_stats(raw_sdf, 'power_output').show(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+----------+----------+--------------+------------+----------+--------+\n",
      "|          timestamp|turbine_id|wind_speed|wind_direction|power_output|      date|    time|\n",
      "+-------------------+----------+----------+--------------+------------+----------+--------+\n",
      "|2022-03-01 00:00:00|        11|       9.1|           269|         2.9|2022-03-01|00:00:00|\n",
      "+-------------------+----------+----------+--------------+------------+----------+--------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tdf = tdf.na.drop()\n",
    "tdf = tdf.withColumn('date', (col('timestamp')).cast('date'))\n",
    "tdf = tdf.withColumn('time', date_format('timestamp', 'HH:mm:ss'))\n",
    "tdf.show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "tdf = tdf.withColumn('date', (col('timestamp')).cast('date'))\n",
    "tdf = tdf.withColumn('time', date_format('timestamp', 'HH:mm:ss'))\n",
    "\n",
    "# tdf.write \\\n",
    "#         .format(\"jdbc\") \\\n",
    "#         .option(\"url\", f\"jdbc:postgresql://{DB_HOST}/{DB_NAME}\") \\\n",
    "#         .option(\"dbtable\", 'turbine_data_raw') \\\n",
    "#         .option(\"user\", DB_USER) \\\n",
    "#         .option(\"password\", DB_PASSWORD) \\\n",
    "#         .option(\"driver\", \"org.postgresql.Driver\") \\\n",
    "#         .save(mode='append')\n",
    "\n",
    "# # Here we assume df has columns ['Date', 'Time', 'TurbineID', 'WindSpeed', 'WindDirection', 'PowerOutput']\n",
    "# # Convert Spark DataFrame to a list of tuples\n",
    "# data_to_insert = [[row.date, row.time, row.turbine_id, row.wind_speed, row.wind_direction, row.power_output, row.timestamp] for row in tdf.collect()]\n",
    "data_to_insert = [[item for item in row] for row in tdf.collect()]\n",
    "\n",
    "insert_query = f\"\"\"\n",
    "    INSERT INTO turbine_data_raw ({', '.join(tdf.columns)})\n",
    "    VALUES %s\n",
    "\"\"\"\n",
    "\n",
    "# # Insert data\n",
    "execute_values(cursor, insert_query, data_to_insert)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[datetime.datetime(2022, 3, 1, 0, 0),\n",
       "  11,\n",
       "  9.1,\n",
       "  269,\n",
       "  2.9,\n",
       "  datetime.date(2022, 3, 1),\n",
       "  '00:00:00'],\n",
       " [datetime.datetime(2022, 3, 1, 0, 0),\n",
       "  12,\n",
       "  11.3,\n",
       "  316,\n",
       "  2.5,\n",
       "  datetime.date(2022, 3, 1),\n",
       "  '00:00:00'],\n",
       " [datetime.datetime(2022, 3, 1, 0, 0),\n",
       "  13,\n",
       "  11.2,\n",
       "  148,\n",
       "  3.7,\n",
       "  datetime.date(2022, 3, 1),\n",
       "  '00:00:00'],\n",
       " [datetime.datetime(2022, 3, 1, 0, 0),\n",
       "  14,\n",
       "  10.7,\n",
       "  97,\n",
       "  1.6,\n",
       "  datetime.date(2022, 3, 1),\n",
       "  '00:00:00'],\n",
       " [datetime.datetime(2022, 3, 1, 0, 0),\n",
       "  15,\n",
       "  11.0,\n",
       "  81,\n",
       "  4.4,\n",
       "  datetime.date(2022, 3, 1),\n",
       "  '00:00:00'],\n",
       " [datetime.datetime(2022, 3, 1, 1, 0),\n",
       "  11,\n",
       "  12.3,\n",
       "  245,\n",
       "  1.8,\n",
       "  datetime.date(2022, 3, 1),\n",
       "  '01:00:00'],\n",
       " [datetime.datetime(2022, 3, 1, 1, 0),\n",
       "  12,\n",
       "  11.0,\n",
       "  293,\n",
       "  2.2,\n",
       "  datetime.date(2022, 3, 1),\n",
       "  '01:00:00'],\n",
       " [datetime.datetime(2022, 3, 1, 1, 0),\n",
       "  13,\n",
       "  11.4,\n",
       "  270,\n",
       "  1.9,\n",
       "  datetime.date(2022, 3, 1),\n",
       "  '01:00:00'],\n",
       " [datetime.datetime(2022, 3, 1, 1, 0),\n",
       "  14,\n",
       "  10.4,\n",
       "  140,\n",
       "  2.3,\n",
       "  datetime.date(2022, 3, 1),\n",
       "  '01:00:00'],\n",
       " [datetime.datetime(2022, 3, 1, 1, 0),\n",
       "  15,\n",
       "  14.6,\n",
       "  283,\n",
       "  4.3,\n",
       "  datetime.date(2022, 3, 1),\n",
       "  '01:00:00'],\n",
       " [datetime.datetime(2022, 3, 1, 2, 0),\n",
       "  11,\n",
       "  14.3,\n",
       "  135,\n",
       "  2.9,\n",
       "  datetime.date(2022, 3, 1),\n",
       "  '02:00:00'],\n",
       " [datetime.datetime(2022, 3, 1, 2, 0),\n",
       "  12,\n",
       "  9.1,\n",
       "  358,\n",
       "  3.2,\n",
       "  datetime.date(2022, 3, 1),\n",
       "  '02:00:00'],\n",
       " [datetime.datetime(2022, 3, 1, 2, 0),\n",
       "  13,\n",
       "  13.7,\n",
       "  67,\n",
       "  3.1,\n",
       "  datetime.date(2022, 3, 1),\n",
       "  '02:00:00'],\n",
       " [datetime.datetime(2022, 3, 1, 2, 0),\n",
       "  14,\n",
       "  9.2,\n",
       "  89,\n",
       "  2.7,\n",
       "  datetime.date(2022, 3, 1),\n",
       "  '02:00:00'],\n",
       " [datetime.datetime(2022, 3, 1, 2, 0),\n",
       "  15,\n",
       "  9.7,\n",
       "  303,\n",
       "  4.5,\n",
       "  datetime.date(2022, 3, 1),\n",
       "  '02:00:00'],\n",
       " [datetime.datetime(2022, 3, 1, 3, 0),\n",
       "  11,\n",
       "  9.6,\n",
       "  171,\n",
       "  2.6,\n",
       "  datetime.date(2022, 3, 1),\n",
       "  '03:00:00'],\n",
       " [datetime.datetime(2022, 3, 1, 3, 0),\n",
       "  12,\n",
       "  11.0,\n",
       "  175,\n",
       "  2.4,\n",
       "  datetime.date(2022, 3, 1),\n",
       "  '03:00:00'],\n",
       " [datetime.datetime(2022, 3, 1, 3, 0),\n",
       "  13,\n",
       "  13.0,\n",
       "  42,\n",
       "  4.1,\n",
       "  datetime.date(2022, 3, 1),\n",
       "  '03:00:00'],\n",
       " [datetime.datetime(2022, 3, 1, 3, 0),\n",
       "  14,\n",
       "  13.6,\n",
       "  198,\n",
       "  3.1,\n",
       "  datetime.date(2022, 3, 1),\n",
       "  '03:00:00'],\n",
       " [datetime.datetime(2022, 3, 1, 3, 0),\n",
       "  15,\n",
       "  9.3,\n",
       "  162,\n",
       "  3.8,\n",
       "  datetime.date(2022, 3, 1),\n",
       "  '03:00:00'],\n",
       " [datetime.datetime(2022, 3, 1, 4, 0),\n",
       "  11,\n",
       "  9.2,\n",
       "  316,\n",
       "  4.2,\n",
       "  datetime.date(2022, 3, 1),\n",
       "  '04:00:00'],\n",
       " [datetime.datetime(2022, 3, 1, 4, 0),\n",
       "  12,\n",
       "  11.8,\n",
       "  170,\n",
       "  3.4,\n",
       "  datetime.date(2022, 3, 1),\n",
       "  '04:00:00'],\n",
       " [datetime.datetime(2022, 3, 1, 4, 0),\n",
       "  13,\n",
       "  14.3,\n",
       "  333,\n",
       "  2.3,\n",
       "  datetime.date(2022, 3, 1),\n",
       "  '04:00:00'],\n",
       " [datetime.datetime(2022, 3, 1, 4, 0),\n",
       "  14,\n",
       "  12.3,\n",
       "  118,\n",
       "  1.9,\n",
       "  datetime.date(2022, 3, 1),\n",
       "  '04:00:00'],\n",
       " [datetime.datetime(2022, 3, 1, 4, 0),\n",
       "  15,\n",
       "  9.6,\n",
       "  213,\n",
       "  4.0,\n",
       "  datetime.date(2022, 3, 1),\n",
       "  '04:00:00'],\n",
       " [datetime.datetime(2022, 3, 1, 5, 0),\n",
       "  11,\n",
       "  14.1,\n",
       "  176,\n",
       "  3.2,\n",
       "  datetime.date(2022, 3, 1),\n",
       "  '05:00:00'],\n",
       " [datetime.datetime(2022, 3, 1, 5, 0),\n",
       "  12,\n",
       "  12.9,\n",
       "  15,\n",
       "  1.6,\n",
       "  datetime.date(2022, 3, 1),\n",
       "  '05:00:00'],\n",
       " [datetime.datetime(2022, 3, 1, 5, 0),\n",
       "  13,\n",
       "  10.6,\n",
       "  186,\n",
       "  2.4,\n",
       "  datetime.date(2022, 3, 1),\n",
       "  '05:00:00'],\n",
       " [datetime.datetime(2022, 3, 1, 5, 0),\n",
       "  14,\n",
       "  12.6,\n",
       "  207,\n",
       "  1.7,\n",
       "  datetime.date(2022, 3, 1),\n",
       "  '05:00:00'],\n",
       " [datetime.datetime(2022, 3, 1, 5, 0),\n",
       "  15,\n",
       "  10.1,\n",
       "  332,\n",
       "  2.0,\n",
       "  datetime.date(2022, 3, 1),\n",
       "  '05:00:00'],\n",
       " [datetime.datetime(2022, 3, 1, 6, 0),\n",
       "  11,\n",
       "  14.5,\n",
       "  251,\n",
       "  3.5,\n",
       "  datetime.date(2022, 3, 1),\n",
       "  '06:00:00'],\n",
       " [datetime.datetime(2022, 3, 1, 6, 0),\n",
       "  12,\n",
       "  12.0,\n",
       "  36,\n",
       "  3.2,\n",
       "  datetime.date(2022, 3, 1),\n",
       "  '06:00:00'],\n",
       " [datetime.datetime(2022, 3, 1, 6, 0),\n",
       "  13,\n",
       "  9.4,\n",
       "  270,\n",
       "  3.2,\n",
       "  datetime.date(2022, 3, 1),\n",
       "  '06:00:00'],\n",
       " [datetime.datetime(2022, 3, 1, 6, 0),\n",
       "  14,\n",
       "  13.6,\n",
       "  27,\n",
       "  2.4,\n",
       "  datetime.date(2022, 3, 1),\n",
       "  '06:00:00'],\n",
       " [datetime.datetime(2022, 3, 1, 6, 0),\n",
       "  15,\n",
       "  14.0,\n",
       "  124,\n",
       "  3.7,\n",
       "  datetime.date(2022, 3, 1),\n",
       "  '06:00:00'],\n",
       " [datetime.datetime(2022, 3, 1, 7, 0),\n",
       "  11,\n",
       "  10.4,\n",
       "  64,\n",
       "  2.5,\n",
       "  datetime.date(2022, 3, 1),\n",
       "  '07:00:00'],\n",
       " [datetime.datetime(2022, 3, 1, 7, 0),\n",
       "  12,\n",
       "  12.3,\n",
       "  103,\n",
       "  4.3,\n",
       "  datetime.date(2022, 3, 1),\n",
       "  '07:00:00'],\n",
       " [datetime.datetime(2022, 3, 1, 7, 0),\n",
       "  13,\n",
       "  11.2,\n",
       "  155,\n",
       "  3.3,\n",
       "  datetime.date(2022, 3, 1),\n",
       "  '07:00:00'],\n",
       " [datetime.datetime(2022, 3, 1, 7, 0),\n",
       "  14,\n",
       "  11.5,\n",
       "  82,\n",
       "  3.1,\n",
       "  datetime.date(2022, 3, 1),\n",
       "  '07:00:00'],\n",
       " [datetime.datetime(2022, 3, 1, 7, 0),\n",
       "  15,\n",
       "  9.1,\n",
       "  278,\n",
       "  2.2,\n",
       "  datetime.date(2022, 3, 1),\n",
       "  '07:00:00'],\n",
       " [datetime.datetime(2022, 3, 1, 8, 0),\n",
       "  11,\n",
       "  13.2,\n",
       "  346,\n",
       "  3.7,\n",
       "  datetime.date(2022, 3, 1),\n",
       "  '08:00:00'],\n",
       " [datetime.datetime(2022, 3, 1, 8, 0),\n",
       "  12,\n",
       "  15.0,\n",
       "  276,\n",
       "  4.5,\n",
       "  datetime.date(2022, 3, 1),\n",
       "  '08:00:00'],\n",
       " [datetime.datetime(2022, 3, 1, 8, 0),\n",
       "  13,\n",
       "  14.4,\n",
       "  134,\n",
       "  2.3,\n",
       "  datetime.date(2022, 3, 1),\n",
       "  '08:00:00'],\n",
       " [datetime.datetime(2022, 3, 1, 8, 0),\n",
       "  14,\n",
       "  11.9,\n",
       "  283,\n",
       "  3.6,\n",
       "  datetime.date(2022, 3, 1),\n",
       "  '08:00:00'],\n",
       " [datetime.datetime(2022, 3, 1, 8, 0),\n",
       "  15,\n",
       "  13.5,\n",
       "  242,\n",
       "  3.9,\n",
       "  datetime.date(2022, 3, 1),\n",
       "  '08:00:00'],\n",
       " [datetime.datetime(2022, 3, 1, 9, 0),\n",
       "  11,\n",
       "  10.0,\n",
       "  322,\n",
       "  2.4,\n",
       "  datetime.date(2022, 3, 1),\n",
       "  '09:00:00'],\n",
       " [datetime.datetime(2022, 3, 1, 9, 0),\n",
       "  12,\n",
       "  10.0,\n",
       "  343,\n",
       "  2.0,\n",
       "  datetime.date(2022, 3, 1),\n",
       "  '09:00:00'],\n",
       " [datetime.datetime(2022, 3, 1, 9, 0),\n",
       "  13,\n",
       "  13.3,\n",
       "  359,\n",
       "  3.8,\n",
       "  datetime.date(2022, 3, 1),\n",
       "  '09:00:00'],\n",
       " [datetime.datetime(2022, 3, 1, 9, 0),\n",
       "  14,\n",
       "  12.1,\n",
       "  216,\n",
       "  4.3,\n",
       "  datetime.date(2022, 3, 1),\n",
       "  '09:00:00'],\n",
       " [datetime.datetime(2022, 3, 1, 9, 0),\n",
       "  15,\n",
       "  9.2,\n",
       "  162,\n",
       "  2.2,\n",
       "  datetime.date(2022, 3, 1),\n",
       "  '09:00:00'],\n",
       " [datetime.datetime(2022, 3, 1, 10, 0),\n",
       "  11,\n",
       "  12.0,\n",
       "  244,\n",
       "  3.5,\n",
       "  datetime.date(2022, 3, 1),\n",
       "  '10:00:00'],\n",
       " [datetime.datetime(2022, 3, 1, 10, 0),\n",
       "  12,\n",
       "  12.8,\n",
       "  61,\n",
       "  4.4,\n",
       "  datetime.date(2022, 3, 1),\n",
       "  '10:00:00'],\n",
       " [datetime.datetime(2022, 3, 1, 10, 0),\n",
       "  13,\n",
       "  11.2,\n",
       "  115,\n",
       "  3.9,\n",
       "  datetime.date(2022, 3, 1),\n",
       "  '10:00:00'],\n",
       " [datetime.datetime(2022, 3, 1, 10, 0),\n",
       "  14,\n",
       "  14.6,\n",
       "  110,\n",
       "  3.2,\n",
       "  datetime.date(2022, 3, 1),\n",
       "  '10:00:00'],\n",
       " [datetime.datetime(2022, 3, 1, 10, 0),\n",
       "  15,\n",
       "  12.0,\n",
       "  159,\n",
       "  2.4,\n",
       "  datetime.date(2022, 3, 1),\n",
       "  '10:00:00'],\n",
       " [datetime.datetime(2022, 3, 1, 11, 0),\n",
       "  11,\n",
       "  14.4,\n",
       "  313,\n",
       "  4.1,\n",
       "  datetime.date(2022, 3, 1),\n",
       "  '11:00:00'],\n",
       " [datetime.datetime(2022, 3, 1, 11, 0),\n",
       "  12,\n",
       "  11.1,\n",
       "  61,\n",
       "  4.2,\n",
       "  datetime.date(2022, 3, 1),\n",
       "  '11:00:00'],\n",
       " [datetime.datetime(2022, 3, 1, 11, 0),\n",
       "  13,\n",
       "  14.1,\n",
       "  172,\n",
       "  4.3,\n",
       "  datetime.date(2022, 3, 1),\n",
       "  '11:00:00'],\n",
       " [datetime.datetime(2022, 3, 1, 11, 0),\n",
       "  14,\n",
       "  10.1,\n",
       "  308,\n",
       "  3.1,\n",
       "  datetime.date(2022, 3, 1),\n",
       "  '11:00:00'],\n",
       " [datetime.datetime(2022, 3, 1, 11, 0),\n",
       "  15,\n",
       "  12.9,\n",
       "  339,\n",
       "  2.5,\n",
       "  datetime.date(2022, 3, 1),\n",
       "  '11:00:00'],\n",
       " [datetime.datetime(2022, 3, 1, 12, 0),\n",
       "  11,\n",
       "  13.1,\n",
       "  77,\n",
       "  3.2,\n",
       "  datetime.date(2022, 3, 1),\n",
       "  '12:00:00'],\n",
       " [datetime.datetime(2022, 3, 1, 12, 0),\n",
       "  12,\n",
       "  13.9,\n",
       "  178,\n",
       "  1.6,\n",
       "  datetime.date(2022, 3, 1),\n",
       "  '12:00:00'],\n",
       " [datetime.datetime(2022, 3, 1, 12, 0),\n",
       "  13,\n",
       "  13.2,\n",
       "  351,\n",
       "  3.2,\n",
       "  datetime.date(2022, 3, 1),\n",
       "  '12:00:00'],\n",
       " [datetime.datetime(2022, 3, 1, 12, 0),\n",
       "  14,\n",
       "  14.9,\n",
       "  173,\n",
       "  3.9,\n",
       "  datetime.date(2022, 3, 1),\n",
       "  '12:00:00'],\n",
       " [datetime.datetime(2022, 3, 1, 12, 0),\n",
       "  15,\n",
       "  13.6,\n",
       "  125,\n",
       "  4.2,\n",
       "  datetime.date(2022, 3, 1),\n",
       "  '12:00:00'],\n",
       " [datetime.datetime(2022, 3, 1, 13, 0),\n",
       "  11,\n",
       "  9.7,\n",
       "  231,\n",
       "  2.1,\n",
       "  datetime.date(2022, 3, 1),\n",
       "  '13:00:00'],\n",
       " [datetime.datetime(2022, 3, 1, 13, 0),\n",
       "  12,\n",
       "  13.1,\n",
       "  254,\n",
       "  2.7,\n",
       "  datetime.date(2022, 3, 1),\n",
       "  '13:00:00'],\n",
       " [datetime.datetime(2022, 3, 1, 13, 0),\n",
       "  13,\n",
       "  13.0,\n",
       "  346,\n",
       "  2.0,\n",
       "  datetime.date(2022, 3, 1),\n",
       "  '13:00:00'],\n",
       " [datetime.datetime(2022, 3, 1, 13, 0),\n",
       "  14,\n",
       "  13.2,\n",
       "  217,\n",
       "  4.3,\n",
       "  datetime.date(2022, 3, 1),\n",
       "  '13:00:00'],\n",
       " [datetime.datetime(2022, 3, 1, 13, 0),\n",
       "  15,\n",
       "  9.4,\n",
       "  91,\n",
       "  2.0,\n",
       "  datetime.date(2022, 3, 1),\n",
       "  '13:00:00'],\n",
       " [datetime.datetime(2022, 3, 1, 14, 0),\n",
       "  11,\n",
       "  11.3,\n",
       "  137,\n",
       "  1.6,\n",
       "  datetime.date(2022, 3, 1),\n",
       "  '14:00:00'],\n",
       " [datetime.datetime(2022, 3, 1, 14, 0),\n",
       "  12,\n",
       "  14.2,\n",
       "  47,\n",
       "  4.4,\n",
       "  datetime.date(2022, 3, 1),\n",
       "  '14:00:00'],\n",
       " [datetime.datetime(2022, 3, 1, 14, 0),\n",
       "  13,\n",
       "  9.8,\n",
       "  358,\n",
       "  3.2,\n",
       "  datetime.date(2022, 3, 1),\n",
       "  '14:00:00'],\n",
       " [datetime.datetime(2022, 3, 1, 14, 0),\n",
       "  14,\n",
       "  9.6,\n",
       "  315,\n",
       "  4.0,\n",
       "  datetime.date(2022, 3, 1),\n",
       "  '14:00:00'],\n",
       " [datetime.datetime(2022, 3, 1, 14, 0),\n",
       "  15,\n",
       "  9.6,\n",
       "  297,\n",
       "  3.8,\n",
       "  datetime.date(2022, 3, 1),\n",
       "  '14:00:00'],\n",
       " [datetime.datetime(2022, 3, 1, 15, 0),\n",
       "  11,\n",
       "  14.7,\n",
       "  62,\n",
       "  1.7,\n",
       "  datetime.date(2022, 3, 1),\n",
       "  '15:00:00'],\n",
       " [datetime.datetime(2022, 3, 1, 15, 0),\n",
       "  12,\n",
       "  14.5,\n",
       "  71,\n",
       "  2.0,\n",
       "  datetime.date(2022, 3, 1),\n",
       "  '15:00:00'],\n",
       " [datetime.datetime(2022, 3, 1, 15, 0),\n",
       "  13,\n",
       "  11.5,\n",
       "  226,\n",
       "  3.6,\n",
       "  datetime.date(2022, 3, 1),\n",
       "  '15:00:00'],\n",
       " [datetime.datetime(2022, 3, 1, 15, 0),\n",
       "  14,\n",
       "  12.9,\n",
       "  147,\n",
       "  2.9,\n",
       "  datetime.date(2022, 3, 1),\n",
       "  '15:00:00'],\n",
       " [datetime.datetime(2022, 3, 1, 15, 0),\n",
       "  15,\n",
       "  11.3,\n",
       "  310,\n",
       "  3.5,\n",
       "  datetime.date(2022, 3, 1),\n",
       "  '15:00:00'],\n",
       " [datetime.datetime(2022, 3, 1, 16, 0),\n",
       "  11,\n",
       "  9.3,\n",
       "  24,\n",
       "  1.9,\n",
       "  datetime.date(2022, 3, 1),\n",
       "  '16:00:00'],\n",
       " [datetime.datetime(2022, 3, 1, 16, 0),\n",
       "  12,\n",
       "  12.8,\n",
       "  233,\n",
       "  3.0,\n",
       "  datetime.date(2022, 3, 1),\n",
       "  '16:00:00'],\n",
       " [datetime.datetime(2022, 3, 1, 16, 0),\n",
       "  13,\n",
       "  11.3,\n",
       "  25,\n",
       "  1.8,\n",
       "  datetime.date(2022, 3, 1),\n",
       "  '16:00:00'],\n",
       " [datetime.datetime(2022, 3, 1, 16, 0),\n",
       "  14,\n",
       "  12.5,\n",
       "  236,\n",
       "  1.6,\n",
       "  datetime.date(2022, 3, 1),\n",
       "  '16:00:00'],\n",
       " [datetime.datetime(2022, 3, 1, 16, 0),\n",
       "  15,\n",
       "  13.5,\n",
       "  253,\n",
       "  4.3,\n",
       "  datetime.date(2022, 3, 1),\n",
       "  '16:00:00'],\n",
       " [datetime.datetime(2022, 3, 1, 17, 0),\n",
       "  11,\n",
       "  14.1,\n",
       "  67,\n",
       "  1.7,\n",
       "  datetime.date(2022, 3, 1),\n",
       "  '17:00:00'],\n",
       " [datetime.datetime(2022, 3, 1, 17, 0),\n",
       "  12,\n",
       "  13.3,\n",
       "  175,\n",
       "  4.0,\n",
       "  datetime.date(2022, 3, 1),\n",
       "  '17:00:00'],\n",
       " [datetime.datetime(2022, 3, 1, 17, 0),\n",
       "  13,\n",
       "  12.6,\n",
       "  31,\n",
       "  3.5,\n",
       "  datetime.date(2022, 3, 1),\n",
       "  '17:00:00'],\n",
       " [datetime.datetime(2022, 3, 1, 17, 0),\n",
       "  14,\n",
       "  13.7,\n",
       "  316,\n",
       "  3.0,\n",
       "  datetime.date(2022, 3, 1),\n",
       "  '17:00:00'],\n",
       " [datetime.datetime(2022, 3, 1, 17, 0),\n",
       "  15,\n",
       "  11.8,\n",
       "  106,\n",
       "  3.1,\n",
       "  datetime.date(2022, 3, 1),\n",
       "  '17:00:00'],\n",
       " [datetime.datetime(2022, 3, 1, 18, 0),\n",
       "  11,\n",
       "  13.6,\n",
       "  310,\n",
       "  3.1,\n",
       "  datetime.date(2022, 3, 1),\n",
       "  '18:00:00'],\n",
       " [datetime.datetime(2022, 3, 1, 18, 0),\n",
       "  12,\n",
       "  12.3,\n",
       "  107,\n",
       "  4.4,\n",
       "  datetime.date(2022, 3, 1),\n",
       "  '18:00:00'],\n",
       " [datetime.datetime(2022, 3, 1, 18, 0),\n",
       "  13,\n",
       "  13.7,\n",
       "  271,\n",
       "  2.0,\n",
       "  datetime.date(2022, 3, 1),\n",
       "  '18:00:00'],\n",
       " [datetime.datetime(2022, 3, 1, 18, 0),\n",
       "  14,\n",
       "  9.6,\n",
       "  273,\n",
       "  2.5,\n",
       "  datetime.date(2022, 3, 1),\n",
       "  '18:00:00'],\n",
       " [datetime.datetime(2022, 3, 1, 18, 0),\n",
       "  15,\n",
       "  13.9,\n",
       "  34,\n",
       "  3.6,\n",
       "  datetime.date(2022, 3, 1),\n",
       "  '18:00:00'],\n",
       " [datetime.datetime(2022, 3, 1, 19, 0),\n",
       "  11,\n",
       "  13.3,\n",
       "  268,\n",
       "  1.5,\n",
       "  datetime.date(2022, 3, 1),\n",
       "  '19:00:00'],\n",
       " [datetime.datetime(2022, 3, 1, 19, 0),\n",
       "  12,\n",
       "  11.9,\n",
       "  358,\n",
       "  1.9,\n",
       "  datetime.date(2022, 3, 1),\n",
       "  '19:00:00'],\n",
       " [datetime.datetime(2022, 3, 1, 19, 0),\n",
       "  13,\n",
       "  9.3,\n",
       "  138,\n",
       "  2.3,\n",
       "  datetime.date(2022, 3, 1),\n",
       "  '19:00:00'],\n",
       " [datetime.datetime(2022, 3, 1, 19, 0),\n",
       "  14,\n",
       "  13.7,\n",
       "  266,\n",
       "  3.5,\n",
       "  datetime.date(2022, 3, 1),\n",
       "  '19:00:00'],\n",
       " [datetime.datetime(2022, 3, 1, 19, 0),\n",
       "  15,\n",
       "  10.3,\n",
       "  237,\n",
       "  4.2,\n",
       "  datetime.date(2022, 3, 1),\n",
       "  '19:00:00'],\n",
       " [datetime.datetime(2022, 3, 1, 20, 0),\n",
       "  11,\n",
       "  13.9,\n",
       "  238,\n",
       "  4.4,\n",
       "  datetime.date(2022, 3, 1),\n",
       "  '20:00:00'],\n",
       " [datetime.datetime(2022, 3, 1, 20, 0),\n",
       "  12,\n",
       "  9.5,\n",
       "  87,\n",
       "  3.9,\n",
       "  datetime.date(2022, 3, 1),\n",
       "  '20:00:00'],\n",
       " [datetime.datetime(2022, 3, 1, 20, 0),\n",
       "  13,\n",
       "  10.9,\n",
       "  312,\n",
       "  2.1,\n",
       "  datetime.date(2022, 3, 1),\n",
       "  '20:00:00'],\n",
       " [datetime.datetime(2022, 3, 1, 20, 0),\n",
       "  14,\n",
       "  12.3,\n",
       "  250,\n",
       "  4.5,\n",
       "  datetime.date(2022, 3, 1),\n",
       "  '20:00:00'],\n",
       " [datetime.datetime(2022, 3, 1, 20, 0),\n",
       "  15,\n",
       "  14.1,\n",
       "  167,\n",
       "  4.4,\n",
       "  datetime.date(2022, 3, 1),\n",
       "  '20:00:00'],\n",
       " [datetime.datetime(2022, 3, 1, 21, 0),\n",
       "  11,\n",
       "  11.4,\n",
       "  47,\n",
       "  3.1,\n",
       "  datetime.date(2022, 3, 1),\n",
       "  '21:00:00'],\n",
       " [datetime.datetime(2022, 3, 1, 21, 0),\n",
       "  12,\n",
       "  12.3,\n",
       "  324,\n",
       "  4.0,\n",
       "  datetime.date(2022, 3, 1),\n",
       "  '21:00:00'],\n",
       " [datetime.datetime(2022, 3, 1, 21, 0),\n",
       "  13,\n",
       "  11.3,\n",
       "  148,\n",
       "  3.4,\n",
       "  datetime.date(2022, 3, 1),\n",
       "  '21:00:00'],\n",
       " [datetime.datetime(2022, 3, 1, 21, 0),\n",
       "  14,\n",
       "  12.7,\n",
       "  267,\n",
       "  3.1,\n",
       "  datetime.date(2022, 3, 1),\n",
       "  '21:00:00'],\n",
       " [datetime.datetime(2022, 3, 1, 21, 0),\n",
       "  15,\n",
       "  10.7,\n",
       "  169,\n",
       "  3.4,\n",
       "  datetime.date(2022, 3, 1),\n",
       "  '21:00:00'],\n",
       " [datetime.datetime(2022, 3, 1, 22, 0),\n",
       "  11,\n",
       "  12.1,\n",
       "  283,\n",
       "  2.0,\n",
       "  datetime.date(2022, 3, 1),\n",
       "  '22:00:00'],\n",
       " [datetime.datetime(2022, 3, 1, 22, 0),\n",
       "  12,\n",
       "  11.8,\n",
       "  175,\n",
       "  2.2,\n",
       "  datetime.date(2022, 3, 1),\n",
       "  '22:00:00'],\n",
       " [datetime.datetime(2022, 3, 1, 22, 0),\n",
       "  13,\n",
       "  11.5,\n",
       "  45,\n",
       "  4.2,\n",
       "  datetime.date(2022, 3, 1),\n",
       "  '22:00:00'],\n",
       " [datetime.datetime(2022, 3, 1, 22, 0),\n",
       "  14,\n",
       "  11.3,\n",
       "  15,\n",
       "  2.2,\n",
       "  datetime.date(2022, 3, 1),\n",
       "  '22:00:00'],\n",
       " [datetime.datetime(2022, 3, 1, 22, 0),\n",
       "  15,\n",
       "  12.0,\n",
       "  314,\n",
       "  2.2,\n",
       "  datetime.date(2022, 3, 1),\n",
       "  '22:00:00'],\n",
       " [datetime.datetime(2022, 3, 1, 23, 0),\n",
       "  11,\n",
       "  13.9,\n",
       "  146,\n",
       "  2.1,\n",
       "  datetime.date(2022, 3, 1),\n",
       "  '23:00:00'],\n",
       " [datetime.datetime(2022, 3, 1, 23, 0),\n",
       "  12,\n",
       "  9.7,\n",
       "  168,\n",
       "  3.8,\n",
       "  datetime.date(2022, 3, 1),\n",
       "  '23:00:00'],\n",
       " [datetime.datetime(2022, 3, 1, 23, 0),\n",
       "  13,\n",
       "  10.2,\n",
       "  107,\n",
       "  3.2,\n",
       "  datetime.date(2022, 3, 1),\n",
       "  '23:00:00'],\n",
       " [datetime.datetime(2022, 3, 1, 23, 0),\n",
       "  14,\n",
       "  10.1,\n",
       "  93,\n",
       "  3.1,\n",
       "  datetime.date(2022, 3, 1),\n",
       "  '23:00:00'],\n",
       " [datetime.datetime(2022, 3, 1, 23, 0),\n",
       "  15,\n",
       "  13.9,\n",
       "  76,\n",
       "  2.3,\n",
       "  datetime.date(2022, 3, 1),\n",
       "  '23:00:00'],\n",
       " [datetime.datetime(2022, 3, 2, 0, 0),\n",
       "  11,\n",
       "  10.4,\n",
       "  259,\n",
       "  2.2,\n",
       "  datetime.date(2022, 3, 2),\n",
       "  '00:00:00'],\n",
       " [datetime.datetime(2022, 3, 2, 0, 0),\n",
       "  12,\n",
       "  11.1,\n",
       "  161,\n",
       "  3.2,\n",
       "  datetime.date(2022, 3, 2),\n",
       "  '00:00:00'],\n",
       " [datetime.datetime(2022, 3, 2, 0, 0),\n",
       "  13,\n",
       "  14.0,\n",
       "  102,\n",
       "  3.9,\n",
       "  datetime.date(2022, 3, 2),\n",
       "  '00:00:00'],\n",
       " [datetime.datetime(2022, 3, 2, 0, 0),\n",
       "  14,\n",
       "  12.2,\n",
       "  293,\n",
       "  3.8,\n",
       "  datetime.date(2022, 3, 2),\n",
       "  '00:00:00'],\n",
       " [datetime.datetime(2022, 3, 2, 0, 0),\n",
       "  15,\n",
       "  9.8,\n",
       "  172,\n",
       "  2.6,\n",
       "  datetime.date(2022, 3, 2),\n",
       "  '00:00:00'],\n",
       " [datetime.datetime(2022, 3, 2, 1, 0),\n",
       "  11,\n",
       "  13.4,\n",
       "  220,\n",
       "  3.9,\n",
       "  datetime.date(2022, 3, 2),\n",
       "  '01:00:00'],\n",
       " [datetime.datetime(2022, 3, 2, 1, 0),\n",
       "  12,\n",
       "  14.7,\n",
       "  169,\n",
       "  4.3,\n",
       "  datetime.date(2022, 3, 2),\n",
       "  '01:00:00'],\n",
       " [datetime.datetime(2022, 3, 2, 1, 0),\n",
       "  13,\n",
       "  11.5,\n",
       "  91,\n",
       "  2.6,\n",
       "  datetime.date(2022, 3, 2),\n",
       "  '01:00:00'],\n",
       " [datetime.datetime(2022, 3, 2, 1, 0),\n",
       "  14,\n",
       "  14.6,\n",
       "  118,\n",
       "  2.4,\n",
       "  datetime.date(2022, 3, 2),\n",
       "  '01:00:00'],\n",
       " [datetime.datetime(2022, 3, 2, 1, 0),\n",
       "  15,\n",
       "  12.6,\n",
       "  65,\n",
       "  3.7,\n",
       "  datetime.date(2022, 3, 2),\n",
       "  '01:00:00'],\n",
       " [datetime.datetime(2022, 3, 2, 2, 0),\n",
       "  11,\n",
       "  11.3,\n",
       "  107,\n",
       "  2.3,\n",
       "  datetime.date(2022, 3, 2),\n",
       "  '02:00:00'],\n",
       " [datetime.datetime(2022, 3, 2, 2, 0),\n",
       "  12,\n",
       "  14.2,\n",
       "  98,\n",
       "  3.0,\n",
       "  datetime.date(2022, 3, 2),\n",
       "  '02:00:00'],\n",
       " [datetime.datetime(2022, 3, 2, 2, 0),\n",
       "  13,\n",
       "  9.2,\n",
       "  267,\n",
       "  4.2,\n",
       "  datetime.date(2022, 3, 2),\n",
       "  '02:00:00'],\n",
       " [datetime.datetime(2022, 3, 2, 2, 0),\n",
       "  14,\n",
       "  14.9,\n",
       "  301,\n",
       "  2.6,\n",
       "  datetime.date(2022, 3, 2),\n",
       "  '02:00:00'],\n",
       " [datetime.datetime(2022, 3, 2, 2, 0),\n",
       "  15,\n",
       "  9.8,\n",
       "  79,\n",
       "  2.9,\n",
       "  datetime.date(2022, 3, 2),\n",
       "  '02:00:00'],\n",
       " [datetime.datetime(2022, 3, 2, 3, 0),\n",
       "  11,\n",
       "  12.1,\n",
       "  193,\n",
       "  3.4,\n",
       "  datetime.date(2022, 3, 2),\n",
       "  '03:00:00'],\n",
       " [datetime.datetime(2022, 3, 2, 3, 0),\n",
       "  12,\n",
       "  12.2,\n",
       "  60,\n",
       "  1.6,\n",
       "  datetime.date(2022, 3, 2),\n",
       "  '03:00:00'],\n",
       " [datetime.datetime(2022, 3, 2, 3, 0),\n",
       "  13,\n",
       "  11.5,\n",
       "  70,\n",
       "  3.4,\n",
       "  datetime.date(2022, 3, 2),\n",
       "  '03:00:00'],\n",
       " [datetime.datetime(2022, 3, 2, 3, 0),\n",
       "  14,\n",
       "  13.2,\n",
       "  32,\n",
       "  3.0,\n",
       "  datetime.date(2022, 3, 2),\n",
       "  '03:00:00'],\n",
       " [datetime.datetime(2022, 3, 2, 3, 0),\n",
       "  15,\n",
       "  12.3,\n",
       "  80,\n",
       "  3.8,\n",
       "  datetime.date(2022, 3, 2),\n",
       "  '03:00:00'],\n",
       " [datetime.datetime(2022, 3, 2, 4, 0),\n",
       "  11,\n",
       "  14.4,\n",
       "  180,\n",
       "  2.6,\n",
       "  datetime.date(2022, 3, 2),\n",
       "  '04:00:00'],\n",
       " [datetime.datetime(2022, 3, 2, 4, 0),\n",
       "  12,\n",
       "  12.0,\n",
       "  285,\n",
       "  3.9,\n",
       "  datetime.date(2022, 3, 2),\n",
       "  '04:00:00'],\n",
       " [datetime.datetime(2022, 3, 2, 4, 0),\n",
       "  13,\n",
       "  10.9,\n",
       "  34,\n",
       "  3.9,\n",
       "  datetime.date(2022, 3, 2),\n",
       "  '04:00:00'],\n",
       " [datetime.datetime(2022, 3, 2, 4, 0),\n",
       "  14,\n",
       "  14.7,\n",
       "  240,\n",
       "  3.0,\n",
       "  datetime.date(2022, 3, 2),\n",
       "  '04:00:00'],\n",
       " [datetime.datetime(2022, 3, 2, 4, 0),\n",
       "  15,\n",
       "  12.1,\n",
       "  171,\n",
       "  2.4,\n",
       "  datetime.date(2022, 3, 2),\n",
       "  '04:00:00'],\n",
       " [datetime.datetime(2022, 3, 2, 5, 0),\n",
       "  11,\n",
       "  11.0,\n",
       "  37,\n",
       "  2.7,\n",
       "  datetime.date(2022, 3, 2),\n",
       "  '05:00:00'],\n",
       " [datetime.datetime(2022, 3, 2, 5, 0),\n",
       "  12,\n",
       "  13.3,\n",
       "  101,\n",
       "  4.3,\n",
       "  datetime.date(2022, 3, 2),\n",
       "  '05:00:00'],\n",
       " [datetime.datetime(2022, 3, 2, 5, 0),\n",
       "  13,\n",
       "  14.3,\n",
       "  138,\n",
       "  4.2,\n",
       "  datetime.date(2022, 3, 2),\n",
       "  '05:00:00'],\n",
       " [datetime.datetime(2022, 3, 2, 5, 0),\n",
       "  14,\n",
       "  14.1,\n",
       "  312,\n",
       "  2.3,\n",
       "  datetime.date(2022, 3, 2),\n",
       "  '05:00:00'],\n",
       " [datetime.datetime(2022, 3, 2, 5, 0),\n",
       "  15,\n",
       "  9.6,\n",
       "  256,\n",
       "  2.1,\n",
       "  datetime.date(2022, 3, 2),\n",
       "  '05:00:00'],\n",
       " [datetime.datetime(2022, 3, 2, 6, 0),\n",
       "  11,\n",
       "  11.9,\n",
       "  87,\n",
       "  4.2,\n",
       "  datetime.date(2022, 3, 2),\n",
       "  '06:00:00'],\n",
       " [datetime.datetime(2022, 3, 2, 6, 0),\n",
       "  12,\n",
       "  11.2,\n",
       "  135,\n",
       "  2.8,\n",
       "  datetime.date(2022, 3, 2),\n",
       "  '06:00:00'],\n",
       " [datetime.datetime(2022, 3, 2, 6, 0),\n",
       "  13,\n",
       "  10.3,\n",
       "  324,\n",
       "  2.0,\n",
       "  datetime.date(2022, 3, 2),\n",
       "  '06:00:00'],\n",
       " [datetime.datetime(2022, 3, 2, 6, 0),\n",
       "  14,\n",
       "  12.9,\n",
       "  237,\n",
       "  3.1,\n",
       "  datetime.date(2022, 3, 2),\n",
       "  '06:00:00'],\n",
       " [datetime.datetime(2022, 3, 2, 6, 0),\n",
       "  15,\n",
       "  9.2,\n",
       "  266,\n",
       "  2.2,\n",
       "  datetime.date(2022, 3, 2),\n",
       "  '06:00:00'],\n",
       " [datetime.datetime(2022, 3, 2, 7, 0),\n",
       "  11,\n",
       "  14.8,\n",
       "  35,\n",
       "  3.1,\n",
       "  datetime.date(2022, 3, 2),\n",
       "  '07:00:00'],\n",
       " [datetime.datetime(2022, 3, 2, 7, 0),\n",
       "  12,\n",
       "  13.3,\n",
       "  307,\n",
       "  1.9,\n",
       "  datetime.date(2022, 3, 2),\n",
       "  '07:00:00'],\n",
       " [datetime.datetime(2022, 3, 2, 7, 0),\n",
       "  13,\n",
       "  13.5,\n",
       "  33,\n",
       "  2.6,\n",
       "  datetime.date(2022, 3, 2),\n",
       "  '07:00:00'],\n",
       " [datetime.datetime(2022, 3, 2, 7, 0),\n",
       "  14,\n",
       "  12.3,\n",
       "  92,\n",
       "  2.6,\n",
       "  datetime.date(2022, 3, 2),\n",
       "  '07:00:00'],\n",
       " [datetime.datetime(2022, 3, 2, 7, 0),\n",
       "  15,\n",
       "  10.7,\n",
       "  305,\n",
       "  2.5,\n",
       "  datetime.date(2022, 3, 2),\n",
       "  '07:00:00'],\n",
       " [datetime.datetime(2022, 3, 2, 8, 0),\n",
       "  11,\n",
       "  13.8,\n",
       "  0,\n",
       "  3.6,\n",
       "  datetime.date(2022, 3, 2),\n",
       "  '08:00:00'],\n",
       " [datetime.datetime(2022, 3, 2, 8, 0),\n",
       "  12,\n",
       "  10.1,\n",
       "  198,\n",
       "  3.0,\n",
       "  datetime.date(2022, 3, 2),\n",
       "  '08:00:00'],\n",
       " [datetime.datetime(2022, 3, 2, 8, 0),\n",
       "  13,\n",
       "  14.6,\n",
       "  331,\n",
       "  4.5,\n",
       "  datetime.date(2022, 3, 2),\n",
       "  '08:00:00'],\n",
       " [datetime.datetime(2022, 3, 2, 8, 0),\n",
       "  14,\n",
       "  9.5,\n",
       "  142,\n",
       "  1.8,\n",
       "  datetime.date(2022, 3, 2),\n",
       "  '08:00:00'],\n",
       " [datetime.datetime(2022, 3, 2, 8, 0),\n",
       "  15,\n",
       "  11.3,\n",
       "  65,\n",
       "  2.3,\n",
       "  datetime.date(2022, 3, 2),\n",
       "  '08:00:00'],\n",
       " [datetime.datetime(2022, 3, 2, 9, 0),\n",
       "  11,\n",
       "  9.8,\n",
       "  167,\n",
       "  3.4,\n",
       "  datetime.date(2022, 3, 2),\n",
       "  '09:00:00'],\n",
       " [datetime.datetime(2022, 3, 2, 9, 0),\n",
       "  12,\n",
       "  13.6,\n",
       "  122,\n",
       "  3.4,\n",
       "  datetime.date(2022, 3, 2),\n",
       "  '09:00:00'],\n",
       " [datetime.datetime(2022, 3, 2, 9, 0),\n",
       "  13,\n",
       "  10.2,\n",
       "  33,\n",
       "  1.8,\n",
       "  datetime.date(2022, 3, 2),\n",
       "  '09:00:00'],\n",
       " [datetime.datetime(2022, 3, 2, 9, 0),\n",
       "  14,\n",
       "  11.5,\n",
       "  72,\n",
       "  3.9,\n",
       "  datetime.date(2022, 3, 2),\n",
       "  '09:00:00'],\n",
       " [datetime.datetime(2022, 3, 2, 9, 0),\n",
       "  15,\n",
       "  9.5,\n",
       "  7,\n",
       "  1.9,\n",
       "  datetime.date(2022, 3, 2),\n",
       "  '09:00:00'],\n",
       " [datetime.datetime(2022, 3, 2, 10, 0),\n",
       "  11,\n",
       "  10.7,\n",
       "  324,\n",
       "  3.9,\n",
       "  datetime.date(2022, 3, 2),\n",
       "  '10:00:00'],\n",
       " [datetime.datetime(2022, 3, 2, 10, 0),\n",
       "  12,\n",
       "  9.3,\n",
       "  359,\n",
       "  3.5,\n",
       "  datetime.date(2022, 3, 2),\n",
       "  '10:00:00'],\n",
       " [datetime.datetime(2022, 3, 2, 10, 0),\n",
       "  13,\n",
       "  13.1,\n",
       "  129,\n",
       "  3.8,\n",
       "  datetime.date(2022, 3, 2),\n",
       "  '10:00:00'],\n",
       " [datetime.datetime(2022, 3, 2, 10, 0),\n",
       "  14,\n",
       "  14.7,\n",
       "  312,\n",
       "  4.4,\n",
       "  datetime.date(2022, 3, 2),\n",
       "  '10:00:00'],\n",
       " [datetime.datetime(2022, 3, 2, 10, 0),\n",
       "  15,\n",
       "  14.4,\n",
       "  309,\n",
       "  3.0,\n",
       "  datetime.date(2022, 3, 2),\n",
       "  '10:00:00'],\n",
       " [datetime.datetime(2022, 3, 2, 11, 0),\n",
       "  11,\n",
       "  12.0,\n",
       "  139,\n",
       "  3.4,\n",
       "  datetime.date(2022, 3, 2),\n",
       "  '11:00:00'],\n",
       " [datetime.datetime(2022, 3, 2, 11, 0),\n",
       "  12,\n",
       "  13.8,\n",
       "  153,\n",
       "  4.4,\n",
       "  datetime.date(2022, 3, 2),\n",
       "  '11:00:00'],\n",
       " [datetime.datetime(2022, 3, 2, 11, 0),\n",
       "  13,\n",
       "  11.8,\n",
       "  146,\n",
       "  3.5,\n",
       "  datetime.date(2022, 3, 2),\n",
       "  '11:00:00'],\n",
       " [datetime.datetime(2022, 3, 2, 11, 0),\n",
       "  14,\n",
       "  10.2,\n",
       "  31,\n",
       "  3.8,\n",
       "  datetime.date(2022, 3, 2),\n",
       "  '11:00:00'],\n",
       " [datetime.datetime(2022, 3, 2, 11, 0),\n",
       "  15,\n",
       "  11.3,\n",
       "  250,\n",
       "  3.9,\n",
       "  datetime.date(2022, 3, 2),\n",
       "  '11:00:00'],\n",
       " [datetime.datetime(2022, 3, 2, 12, 0),\n",
       "  11,\n",
       "  9.5,\n",
       "  25,\n",
       "  3.2,\n",
       "  datetime.date(2022, 3, 2),\n",
       "  '12:00:00'],\n",
       " [datetime.datetime(2022, 3, 2, 12, 0),\n",
       "  12,\n",
       "  13.8,\n",
       "  127,\n",
       "  4.1,\n",
       "  datetime.date(2022, 3, 2),\n",
       "  '12:00:00'],\n",
       " [datetime.datetime(2022, 3, 2, 12, 0),\n",
       "  13,\n",
       "  10.5,\n",
       "  282,\n",
       "  4.2,\n",
       "  datetime.date(2022, 3, 2),\n",
       "  '12:00:00'],\n",
       " [datetime.datetime(2022, 3, 2, 12, 0),\n",
       "  14,\n",
       "  13.7,\n",
       "  199,\n",
       "  2.6,\n",
       "  datetime.date(2022, 3, 2),\n",
       "  '12:00:00'],\n",
       " [datetime.datetime(2022, 3, 2, 12, 0),\n",
       "  15,\n",
       "  11.0,\n",
       "  300,\n",
       "  1.8,\n",
       "  datetime.date(2022, 3, 2),\n",
       "  '12:00:00'],\n",
       " [datetime.datetime(2022, 3, 2, 13, 0),\n",
       "  11,\n",
       "  14.2,\n",
       "  73,\n",
       "  4.5,\n",
       "  datetime.date(2022, 3, 2),\n",
       "  '13:00:00'],\n",
       " [datetime.datetime(2022, 3, 2, 13, 0),\n",
       "  12,\n",
       "  14.7,\n",
       "  201,\n",
       "  3.4,\n",
       "  datetime.date(2022, 3, 2),\n",
       "  '13:00:00'],\n",
       " [datetime.datetime(2022, 3, 2, 13, 0),\n",
       "  13,\n",
       "  9.0,\n",
       "  296,\n",
       "  3.1,\n",
       "  datetime.date(2022, 3, 2),\n",
       "  '13:00:00'],\n",
       " [datetime.datetime(2022, 3, 2, 13, 0),\n",
       "  14,\n",
       "  12.7,\n",
       "  150,\n",
       "  2.7,\n",
       "  datetime.date(2022, 3, 2),\n",
       "  '13:00:00'],\n",
       " [datetime.datetime(2022, 3, 2, 13, 0),\n",
       "  15,\n",
       "  14.7,\n",
       "  257,\n",
       "  2.9,\n",
       "  datetime.date(2022, 3, 2),\n",
       "  '13:00:00'],\n",
       " [datetime.datetime(2022, 3, 2, 14, 0),\n",
       "  11,\n",
       "  12.5,\n",
       "  278,\n",
       "  3.4,\n",
       "  datetime.date(2022, 3, 2),\n",
       "  '14:00:00'],\n",
       " [datetime.datetime(2022, 3, 2, 14, 0),\n",
       "  12,\n",
       "  14.8,\n",
       "  310,\n",
       "  3.8,\n",
       "  datetime.date(2022, 3, 2),\n",
       "  '14:00:00'],\n",
       " [datetime.datetime(2022, 3, 2, 14, 0),\n",
       "  13,\n",
       "  12.7,\n",
       "  252,\n",
       "  2.3,\n",
       "  datetime.date(2022, 3, 2),\n",
       "  '14:00:00'],\n",
       " [datetime.datetime(2022, 3, 2, 14, 0),\n",
       "  14,\n",
       "  11.5,\n",
       "  276,\n",
       "  3.4,\n",
       "  datetime.date(2022, 3, 2),\n",
       "  '14:00:00'],\n",
       " [datetime.datetime(2022, 3, 2, 14, 0),\n",
       "  15,\n",
       "  12.1,\n",
       "  341,\n",
       "  3.2,\n",
       "  datetime.date(2022, 3, 2),\n",
       "  '14:00:00'],\n",
       " [datetime.datetime(2022, 3, 2, 15, 0),\n",
       "  11,\n",
       "  13.7,\n",
       "  223,\n",
       "  3.8,\n",
       "  datetime.date(2022, 3, 2),\n",
       "  '15:00:00'],\n",
       " [datetime.datetime(2022, 3, 2, 15, 0),\n",
       "  12,\n",
       "  9.3,\n",
       "  337,\n",
       "  2.5,\n",
       "  datetime.date(2022, 3, 2),\n",
       "  '15:00:00'],\n",
       " [datetime.datetime(2022, 3, 2, 15, 0),\n",
       "  13,\n",
       "  11.7,\n",
       "  205,\n",
       "  3.4,\n",
       "  datetime.date(2022, 3, 2),\n",
       "  '15:00:00'],\n",
       " [datetime.datetime(2022, 3, 2, 15, 0),\n",
       "  14,\n",
       "  14.8,\n",
       "  345,\n",
       "  3.0,\n",
       "  datetime.date(2022, 3, 2),\n",
       "  '15:00:00'],\n",
       " [datetime.datetime(2022, 3, 2, 15, 0),\n",
       "  15,\n",
       "  11.2,\n",
       "  13,\n",
       "  3.7,\n",
       "  datetime.date(2022, 3, 2),\n",
       "  '15:00:00'],\n",
       " [datetime.datetime(2022, 3, 2, 16, 0),\n",
       "  11,\n",
       "  11.1,\n",
       "  149,\n",
       "  4.1,\n",
       "  datetime.date(2022, 3, 2),\n",
       "  '16:00:00'],\n",
       " [datetime.datetime(2022, 3, 2, 16, 0),\n",
       "  12,\n",
       "  14.5,\n",
       "  230,\n",
       "  2.5,\n",
       "  datetime.date(2022, 3, 2),\n",
       "  '16:00:00'],\n",
       " [datetime.datetime(2022, 3, 2, 16, 0),\n",
       "  13,\n",
       "  14.8,\n",
       "  144,\n",
       "  1.9,\n",
       "  datetime.date(2022, 3, 2),\n",
       "  '16:00:00'],\n",
       " [datetime.datetime(2022, 3, 2, 16, 0),\n",
       "  14,\n",
       "  14.4,\n",
       "  232,\n",
       "  4.1,\n",
       "  datetime.date(2022, 3, 2),\n",
       "  '16:00:00'],\n",
       " [datetime.datetime(2022, 3, 2, 16, 0),\n",
       "  15,\n",
       "  9.2,\n",
       "  261,\n",
       "  2.3,\n",
       "  datetime.date(2022, 3, 2),\n",
       "  '16:00:00'],\n",
       " [datetime.datetime(2022, 3, 2, 17, 0),\n",
       "  11,\n",
       "  13.7,\n",
       "  11,\n",
       "  3.4,\n",
       "  datetime.date(2022, 3, 2),\n",
       "  '17:00:00'],\n",
       " [datetime.datetime(2022, 3, 2, 17, 0),\n",
       "  12,\n",
       "  14.5,\n",
       "  186,\n",
       "  2.4,\n",
       "  datetime.date(2022, 3, 2),\n",
       "  '17:00:00'],\n",
       " [datetime.datetime(2022, 3, 2, 17, 0),\n",
       "  13,\n",
       "  12.3,\n",
       "  177,\n",
       "  2.5,\n",
       "  datetime.date(2022, 3, 2),\n",
       "  '17:00:00'],\n",
       " [datetime.datetime(2022, 3, 2, 17, 0),\n",
       "  14,\n",
       "  13.3,\n",
       "  52,\n",
       "  1.8,\n",
       "  datetime.date(2022, 3, 2),\n",
       "  '17:00:00'],\n",
       " [datetime.datetime(2022, 3, 2, 17, 0),\n",
       "  15,\n",
       "  11.5,\n",
       "  207,\n",
       "  3.6,\n",
       "  datetime.date(2022, 3, 2),\n",
       "  '17:00:00'],\n",
       " [datetime.datetime(2022, 3, 2, 18, 0),\n",
       "  11,\n",
       "  11.8,\n",
       "  126,\n",
       "  1.8,\n",
       "  datetime.date(2022, 3, 2),\n",
       "  '18:00:00'],\n",
       " [datetime.datetime(2022, 3, 2, 18, 0),\n",
       "  12,\n",
       "  12.6,\n",
       "  126,\n",
       "  1.6,\n",
       "  datetime.date(2022, 3, 2),\n",
       "  '18:00:00'],\n",
       " [datetime.datetime(2022, 3, 2, 18, 0),\n",
       "  13,\n",
       "  14.0,\n",
       "  126,\n",
       "  2.6,\n",
       "  datetime.date(2022, 3, 2),\n",
       "  '18:00:00'],\n",
       " [datetime.datetime(2022, 3, 2, 18, 0),\n",
       "  14,\n",
       "  11.2,\n",
       "  255,\n",
       "  1.9,\n",
       "  datetime.date(2022, 3, 2),\n",
       "  '18:00:00'],\n",
       " [datetime.datetime(2022, 3, 2, 18, 0),\n",
       "  15,\n",
       "  13.6,\n",
       "  297,\n",
       "  3.5,\n",
       "  datetime.date(2022, 3, 2),\n",
       "  '18:00:00'],\n",
       " [datetime.datetime(2022, 3, 2, 19, 0),\n",
       "  11,\n",
       "  10.1,\n",
       "  354,\n",
       "  4.2,\n",
       "  datetime.date(2022, 3, 2),\n",
       "  '19:00:00'],\n",
       " [datetime.datetime(2022, 3, 2, 19, 0),\n",
       "  12,\n",
       "  14.1,\n",
       "  19,\n",
       "  2.7,\n",
       "  datetime.date(2022, 3, 2),\n",
       "  '19:00:00'],\n",
       " [datetime.datetime(2022, 3, 2, 19, 0),\n",
       "  13,\n",
       "  12.3,\n",
       "  176,\n",
       "  4.3,\n",
       "  datetime.date(2022, 3, 2),\n",
       "  '19:00:00'],\n",
       " [datetime.datetime(2022, 3, 2, 19, 0),\n",
       "  14,\n",
       "  9.5,\n",
       "  309,\n",
       "  2.7,\n",
       "  datetime.date(2022, 3, 2),\n",
       "  '19:00:00'],\n",
       " [datetime.datetime(2022, 3, 2, 19, 0),\n",
       "  15,\n",
       "  11.6,\n",
       "  186,\n",
       "  2.8,\n",
       "  datetime.date(2022, 3, 2),\n",
       "  '19:00:00'],\n",
       " [datetime.datetime(2022, 3, 2, 20, 0),\n",
       "  11,\n",
       "  14.5,\n",
       "  169,\n",
       "  1.9,\n",
       "  datetime.date(2022, 3, 2),\n",
       "  '20:00:00'],\n",
       " [datetime.datetime(2022, 3, 2, 20, 0),\n",
       "  12,\n",
       "  9.8,\n",
       "  98,\n",
       "  2.7,\n",
       "  datetime.date(2022, 3, 2),\n",
       "  '20:00:00'],\n",
       " [datetime.datetime(2022, 3, 2, 20, 0),\n",
       "  13,\n",
       "  11.0,\n",
       "  352,\n",
       "  4.1,\n",
       "  datetime.date(2022, 3, 2),\n",
       "  '20:00:00'],\n",
       " [datetime.datetime(2022, 3, 2, 20, 0),\n",
       "  14,\n",
       "  12.3,\n",
       "  320,\n",
       "  2.7,\n",
       "  datetime.date(2022, 3, 2),\n",
       "  '20:00:00'],\n",
       " [datetime.datetime(2022, 3, 2, 20, 0),\n",
       "  15,\n",
       "  13.5,\n",
       "  17,\n",
       "  3.7,\n",
       "  datetime.date(2022, 3, 2),\n",
       "  '20:00:00'],\n",
       " [datetime.datetime(2022, 3, 2, 21, 0),\n",
       "  11,\n",
       "  12.2,\n",
       "  85,\n",
       "  3.0,\n",
       "  datetime.date(2022, 3, 2),\n",
       "  '21:00:00'],\n",
       " [datetime.datetime(2022, 3, 2, 21, 0),\n",
       "  12,\n",
       "  15.0,\n",
       "  211,\n",
       "  2.8,\n",
       "  datetime.date(2022, 3, 2),\n",
       "  '21:00:00'],\n",
       " [datetime.datetime(2022, 3, 2, 21, 0),\n",
       "  13,\n",
       "  12.2,\n",
       "  302,\n",
       "  4.1,\n",
       "  datetime.date(2022, 3, 2),\n",
       "  '21:00:00'],\n",
       " [datetime.datetime(2022, 3, 2, 21, 0),\n",
       "  14,\n",
       "  12.9,\n",
       "  218,\n",
       "  1.8,\n",
       "  datetime.date(2022, 3, 2),\n",
       "  '21:00:00'],\n",
       " [datetime.datetime(2022, 3, 2, 21, 0),\n",
       "  15,\n",
       "  12.0,\n",
       "  341,\n",
       "  3.6,\n",
       "  datetime.date(2022, 3, 2),\n",
       "  '21:00:00'],\n",
       " [datetime.datetime(2022, 3, 2, 22, 0),\n",
       "  11,\n",
       "  9.9,\n",
       "  41,\n",
       "  2.8,\n",
       "  datetime.date(2022, 3, 2),\n",
       "  '22:00:00'],\n",
       " [datetime.datetime(2022, 3, 2, 22, 0),\n",
       "  12,\n",
       "  9.4,\n",
       "  45,\n",
       "  1.8,\n",
       "  datetime.date(2022, 3, 2),\n",
       "  '22:00:00'],\n",
       " [datetime.datetime(2022, 3, 2, 22, 0),\n",
       "  13,\n",
       "  13.2,\n",
       "  320,\n",
       "  3.0,\n",
       "  datetime.date(2022, 3, 2),\n",
       "  '22:00:00'],\n",
       " [datetime.datetime(2022, 3, 2, 22, 0),\n",
       "  14,\n",
       "  10.3,\n",
       "  79,\n",
       "  3.7,\n",
       "  datetime.date(2022, 3, 2),\n",
       "  '22:00:00'],\n",
       " [datetime.datetime(2022, 3, 2, 22, 0),\n",
       "  15,\n",
       "  11.9,\n",
       "  160,\n",
       "  3.7,\n",
       "  datetime.date(2022, 3, 2),\n",
       "  '22:00:00'],\n",
       " [datetime.datetime(2022, 3, 2, 23, 0),\n",
       "  11,\n",
       "  9.7,\n",
       "  350,\n",
       "  2.8,\n",
       "  datetime.date(2022, 3, 2),\n",
       "  '23:00:00'],\n",
       " [datetime.datetime(2022, 3, 2, 23, 0),\n",
       "  12,\n",
       "  11.0,\n",
       "  276,\n",
       "  2.0,\n",
       "  datetime.date(2022, 3, 2),\n",
       "  '23:00:00'],\n",
       " [datetime.datetime(2022, 3, 2, 23, 0),\n",
       "  13,\n",
       "  11.2,\n",
       "  239,\n",
       "  4.3,\n",
       "  datetime.date(2022, 3, 2),\n",
       "  '23:00:00'],\n",
       " [datetime.datetime(2022, 3, 2, 23, 0),\n",
       "  14,\n",
       "  10.9,\n",
       "  313,\n",
       "  3.3,\n",
       "  datetime.date(2022, 3, 2),\n",
       "  '23:00:00'],\n",
       " [datetime.datetime(2022, 3, 2, 23, 0),\n",
       "  15,\n",
       "  11.6,\n",
       "  303,\n",
       "  3.8,\n",
       "  datetime.date(2022, 3, 2),\n",
       "  '23:00:00']]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "data_to_insert = [[item for item in row] for row in tdf.collect()]\n",
    "data_to_insert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    INSERT INTO tol (timestamp, turbine_id, wind_speed, wind_direction, power_output, date, time)\n",
      "    VALUES %s\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "# get_last_times()\n",
    "print(f\"\"\"\n",
    "    INSERT INTO tol ({', '.join(tdf.columns)})\n",
    "    VALUES %s\n",
    "    \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+----------+----------+--------------+------------+----------+--------+\n",
      "|          timestamp|turbine_id|wind_speed|wind_direction|power_output|      date|    time|\n",
      "+-------------------+----------+----------+--------------+------------+----------+--------+\n",
      "|2022-03-01 03:00:00|         1|      10.5|            61|         1.8|2022-03-01|03:00:00|\n",
      "|2022-03-02 03:00:00|         1|      10.0|           256|         4.4|2022-03-02|03:00:00|\n",
      "|2022-03-03 03:00:00|         1|      13.1|            80|         1.9|2022-03-03|03:00:00|\n",
      "|2022-03-04 03:00:00|         1|      12.3|           237|         4.0|2022-03-04|03:00:00|\n",
      "|2022-03-05 03:00:00|         1|      13.5|            51|         2.9|2022-03-05|03:00:00|\n",
      "|2022-03-06 03:00:00|         1|      14.3|           248|         1.6|2022-03-06|03:00:00|\n",
      "|2022-03-07 03:00:00|         1|      13.4|            96|         3.5|2022-03-07|03:00:00|\n",
      "|2022-03-08 03:00:00|         1|      10.2|           284|         3.8|2022-03-08|03:00:00|\n",
      "|2022-03-09 03:00:00|         1|      12.4|            51|         1.6|2022-03-09|03:00:00|\n",
      "|2022-03-10 03:00:00|         1|      12.7|            87|         2.6|2022-03-10|03:00:00|\n",
      "|2022-03-11 03:00:00|         1|      14.1|            59|         2.2|2022-03-11|03:00:00|\n",
      "|2022-03-12 03:00:00|         1|      12.0|           144|         2.5|2022-03-12|03:00:00|\n",
      "|2022-03-13 03:00:00|         1|      13.0|           241|         2.7|2022-03-13|03:00:00|\n",
      "|2022-03-14 03:00:00|         1|      12.2|           343|         2.8|2022-03-14|03:00:00|\n",
      "|2022-03-15 03:00:00|         1|      12.1|             7|         3.6|2022-03-15|03:00:00|\n",
      "|2022-03-16 03:00:00|         1|       9.6|            99|         3.7|2022-03-16|03:00:00|\n",
      "|2022-03-17 03:00:00|         1|      10.1|           297|         3.8|2022-03-17|03:00:00|\n",
      "|2022-03-18 03:00:00|         1|      14.8|           212|         2.5|2022-03-18|03:00:00|\n",
      "|2022-03-19 03:00:00|         1|      13.8|            10|         2.6|2022-03-19|03:00:00|\n",
      "|2022-03-20 03:00:00|         1|       9.7|           231|         2.1|2022-03-20|03:00:00|\n",
      "+-------------------+----------+----------+--------------+------------+----------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sdf_raw.filter((col('turbine_id') == 1) & (col('time') == '03:00:00')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+----------+----------+--------------+------------+----------+--------+------------------------+------------------------+\n",
      "|          timestamp|turbine_id|wind_speed|wind_direction|power_output|      date|    time|lower_bound_power_output|upper_bound_power_output|\n",
      "+-------------------+----------+----------+--------------+------------+----------+--------+------------------------+------------------------+\n",
      "|2022-03-08 00:00:00|        10|      11.3|            56|         4.3|2022-03-08|00:00:00|       1.333915010562099|       4.259633376534676|\n",
      "|2022-03-10 00:00:00|        10|      11.5|           314|         4.5|2022-03-10|00:00:00|       1.333915010562099|       4.259633376534676|\n",
      "|2022-03-07 00:00:00|        15|      12.9|           318|         1.5|2022-03-07|00:00:00|       1.531087632359274|       5.010847851511695|\n",
      "|2022-03-13 01:00:00|         4|      13.8|            79|         4.5|2022-03-13|01:00:00|      0.8914865721928433|       4.441846761140489|\n",
      "|2022-03-02 03:00:00|         1|      10.0|           256|         4.4|2022-03-02|03:00:00|      1.1062706309614114|      4.2227616271031065|\n",
      "|2022-03-13 03:00:00|         4|      12.5|           318|         4.4|2022-03-13|03:00:00|      1.4138305528646655|       4.308750092296625|\n",
      "|2022-03-06 03:00:00|        13|      10.0|           155|         1.7|2022-03-06|03:00:00|      1.7304164092341396|       4.727648106894893|\n",
      "|2022-03-11 04:00:00|        10|      12.6|           356|         4.4|2022-03-11|04:00:00|       1.116753342130468|      4.2574402062566294|\n",
      "|2022-03-13 04:00:00|        13|      11.2|           236|         4.2|2022-03-13|04:00:00|      1.3110521756786992|       4.050238146901945|\n",
      "|2022-03-04 05:00:00|         1|      12.2|           286|         1.5|2022-03-04|05:00:00|      1.5798062052002784|      4.3556776657674625|\n",
      "|2022-03-19 05:00:00|         5|      12.4|           133|         4.5|2022-03-19|05:00:00|      1.8794489079737542|       4.456034962993988|\n",
      "|2022-03-24 05:00:00|        10|      13.7|           216|         4.4|2022-03-24|05:00:00|      1.3607507703085793|        4.10376535872368|\n",
      "|2022-03-07 05:00:00|        11|       9.9|           251|         4.5|2022-03-07|05:00:00|      1.2410460348960712|       4.371857190910381|\n",
      "|2022-03-07 06:00:00|         7|      14.3|            91|         4.3|2022-03-07|06:00:00|       1.071157164161042|       4.290133158419604|\n",
      "|2022-03-13 06:00:00|        14|      12.9|           165|         1.6|2022-03-13|06:00:00|      1.6875540676583929|       4.654381416212574|\n",
      "|2022-03-08 07:00:00|         2|      12.8|           351|         1.7|2022-03-08|07:00:00|       1.741818550083613|       4.690439514432515|\n",
      "|2022-03-25 07:00:00|         2|      11.2|            26|         1.6|2022-03-25|07:00:00|       1.741818550083613|       4.690439514432515|\n",
      "|2022-03-26 07:00:00|         2|      12.9|           184|         1.6|2022-03-26|07:00:00|       1.741818550083613|       4.690439514432515|\n",
      "|2022-03-19 07:00:00|         4|      11.5|           210|         4.4|2022-03-19|07:00:00|       1.048577988773842|       4.377228462839063|\n",
      "|2022-03-19 08:00:00|         7|      10.6|           338|         1.5|2022-03-19|08:00:00|        1.51148510425815|       4.494966508645076|\n",
      "+-------------------+----------+----------+--------------+------------+----------+--------+------------------------+------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "detdf = detectAnomalies(sdf, 'power_output')\n",
    "detdf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+--------------------+----------------+----------------+\n",
      "|turbine_id|      date|average_power_output|min_power_output|max_power_output|\n",
      "+----------+----------+--------------------+----------------+----------------+\n",
      "|         1|2022-03-01|  2.9749999999999996|             1.6|             4.4|\n",
      "|         1|2022-03-02|  3.2375000000000003|             1.9|             4.5|\n",
      "|         1|2022-03-03|  2.9250000000000007|             1.6|             4.4|\n",
      "|         1|2022-03-04|  2.9875000000000007|             1.5|             4.4|\n",
      "|         1|2022-03-05|  3.2458333333333322|             1.6|             4.3|\n",
      "|         1|2022-03-06|  2.9583333333333335|             1.5|             4.4|\n",
      "|         1|2022-03-07|   3.283333333333333|             1.9|             4.3|\n",
      "|         1|2022-03-08|  3.2916666666666665|             1.7|             4.5|\n",
      "|         1|2022-03-09|  2.8375000000000004|             1.5|             4.5|\n",
      "|         1|2022-03-10|   3.120833333333333|             1.7|             4.3|\n",
      "|         1|2022-03-11|  2.5750000000000006|             1.5|             4.4|\n",
      "|         1|2022-03-12|   2.820833333333334|             1.6|             4.2|\n",
      "|         1|2022-03-13|   2.904166666666667|             1.5|             4.5|\n",
      "|         1|2022-03-14|               3.025|             1.6|             4.3|\n",
      "|         1|2022-03-15|  3.0125000000000006|             1.6|             4.4|\n",
      "|         1|2022-03-16|  2.9791666666666665|             1.6|             4.4|\n",
      "|         1|2022-03-17|  3.0208333333333335|             1.6|             4.4|\n",
      "|         1|2022-03-18|  3.1041666666666665|             1.6|             4.5|\n",
      "|         1|2022-03-19|               3.125|             1.8|             4.5|\n",
      "|         1|2022-03-20|   2.716666666666667|             1.6|             3.8|\n",
      "+----------+----------+--------------------+----------------+----------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cdf = calculateDailyStats(sdf, 'power_output')\n",
    "cdf[cdf['turbine_id'] == 1].orderBy('date').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wind_speed\n",
      "+-------------------+----------+----------+--------------+------------+----------+--------+\n",
      "|          timestamp|turbine_id|wind_speed|wind_direction|power_output|      date|    time|\n",
      "+-------------------+----------+----------+--------------+------------+----------+--------+\n",
      "|2022-03-01 00:00:00|         1|      11.8|           169|         2.7|2022-03-01|00:00:00|\n",
      "|2022-03-01 01:00:00|         1|      11.6|           152|         4.4|2022-03-01|01:00:00|\n",
      "+-------------------+----------+----------+--------------+------------+----------+--------+\n",
      "only showing top 2 rows\n",
      "\n",
      "wind_direction\n",
      "+-------------------+----------+----------+--------------+------------+----------+--------+\n",
      "|          timestamp|turbine_id|wind_speed|wind_direction|power_output|      date|    time|\n",
      "+-------------------+----------+----------+--------------+------------+----------+--------+\n",
      "|2022-03-01 00:00:00|         1|      11.8|         169.0|         2.7|2022-03-01|00:00:00|\n",
      "|2022-03-01 01:00:00|         1|      11.6|         152.0|         4.4|2022-03-01|01:00:00|\n",
      "+-------------------+----------+----------+--------------+------------+----------+--------+\n",
      "only showing top 2 rows\n",
      "\n",
      "power_output\n",
      "+-------------------+----------+----------+--------------+------------+----------+--------+\n",
      "|          timestamp|turbine_id|wind_speed|wind_direction|power_output|      date|    time|\n",
      "+-------------------+----------+----------+--------------+------------+----------+--------+\n",
      "|2022-03-01 00:00:00|         1|      11.8|           169|         2.7|2022-03-01|00:00:00|\n",
      "|2022-03-01 01:00:00|         1|      11.6|           152|         4.4|2022-03-01|01:00:00|\n",
      "+-------------------+----------+----------+--------------+------------+----------+--------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "columns = ['wind_speed', 'wind_direction', 'power_output']\n",
    "for column in columns:\n",
    "    print(column)\n",
    "    fillNAsWithMean(sdf, column).show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = sdf.pandas_api()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.dataframe.DataFrame"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(sdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['timestamp', 'turbine_id', 'wind_speed', 'wind_direction',\n",
       "       'power_output', 'date', 'time'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+------------------+---------+---------+\n",
      "|turbine_id|      date|     average_power|min_power|max_power|\n",
      "+----------+----------+------------------+---------+---------+\n",
      "|         3|2022-03-23| 2.795833333333333|      1.5|      4.2|\n",
      "|         2|2022-03-26|2.9875000000000007|      1.5|      4.5|\n",
      "|         3|2022-03-21| 3.341666666666667|      1.5|      4.4|\n",
      "|         3|2022-03-16| 2.904166666666667|      1.8|      4.3|\n",
      "|         5|2022-03-06| 3.008333333333334|      1.5|      4.4|\n",
      "|         2|2022-03-31| 3.229166666666666|      1.6|      4.4|\n",
      "|         1|2022-03-28|3.0750000000000006|      1.6|      4.5|\n",
      "|         1|2022-03-12| 2.820833333333334|      1.6|      4.2|\n",
      "|         5|2022-03-03| 2.941666666666667|      1.6|      4.3|\n",
      "|         3|2022-03-24| 3.158333333333333|      1.9|      4.4|\n",
      "|         2|2022-03-09|            2.9625|      1.5|      4.4|\n",
      "|         5|2022-03-08|3.2791666666666663|      2.1|      4.5|\n",
      "|         4|2022-03-02| 3.108333333333333|      1.5|      4.2|\n",
      "|         4|2022-03-01| 3.020833333333334|      1.5|      4.4|\n",
      "|         2|2022-03-07|3.0000000000000004|      1.7|      4.4|\n",
      "|         1|2022-03-11|2.5750000000000006|      1.5|      4.4|\n",
      "|         5|2022-03-02|              3.25|      1.9|      4.4|\n",
      "|         4|2022-03-18|2.9541666666666675|      1.6|      4.5|\n",
      "|         5|2022-03-21|2.7208333333333337|      1.6|      4.2|\n",
      "|         2|2022-03-28| 2.858333333333333|      1.6|      4.5|\n",
      "+----------+----------+------------------+---------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2.988575268817212"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Data Cleaning\n",
    "df_clean = sdf.na.drop()  # Drops rows with missing values\n",
    "# Assuming wind_speed and power_output are the columns of interest\n",
    "stats = df_clean.select(\n",
    "    mean(col('power_output')).alias('mean'),\n",
    "    stddev(col('power_output')).alias('stddev')\n",
    ").collect()\n",
    "\n",
    "mean_power = stats[0]['mean']\n",
    "stddev_power = stats[0]['stddev']\n",
    "\n",
    "# Anomaly Detection\n",
    "df_anomalies = df_clean.withColumn('z_score', (col('power_output') - mean_power) / stddev_power)\n",
    "df_anomalies = df_anomalies.filter(abs(col('z_score')) > 2)\n",
    "\n",
    "# Summary Statistics\n",
    "df_summary = df_clean.groupBy('turbine_id', 'date').agg(\n",
    "    mean(col('power_output')).alias('average_power'),\n",
    "    min(col('power_output')).alias('min_power'),\n",
    "    max(col('power_output')).alias('max_power')\n",
    ")\n",
    "\n",
    "df_summary.show()\n",
    "mean_power\n",
    "# # Store Processed Data\n",
    "# # Replace `your_table` with your actual table name and configure the database settings\n",
    "# df_clean.write.format('jdbc').option('url', 'jdbc:postgresql://dbserver').option('dbtable', 'your_table').save()\n",
    "\n",
    "# # Store Summary Statistics\n",
    "# df_summary.write.format('jdbc').option('url', 'jdbc:postgresql://dbserver').option('dbtable', 'summary_table').save()\n",
    "\n",
    "# # Close Spark Session\n",
    "# spark.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "ename": "ConnectionRefusedError",
     "evalue": "[WinError 10061] No connection could be made because the target machine actively refused it",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mConnectionResetError\u001b[0m                      Traceback (most recent call last)",
      "File \u001b[1;32mc:\\project\\pyspark\\venv\\lib\\site-packages\\py4j\\clientserver.py:503\u001b[0m, in \u001b[0;36mClientServerConnection.send_command\u001b[1;34m(self, command)\u001b[0m\n\u001b[0;32m    502\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 503\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msocket\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msendall\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcommand\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    504\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[1;31mConnectionResetError\u001b[0m: [WinError 10054] An existing connection was forcibly closed by the remote host",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mPy4JNetworkError\u001b[0m                          Traceback (most recent call last)",
      "File \u001b[1;32mc:\\project\\pyspark\\venv\\lib\\site-packages\\py4j\\java_gateway.py:1038\u001b[0m, in \u001b[0;36mGatewayClient.send_command\u001b[1;34m(self, command, retry, binary)\u001b[0m\n\u001b[0;32m   1037\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1038\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mconnection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend_command\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcommand\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1039\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m binary:\n",
      "File \u001b[1;32mc:\\project\\pyspark\\venv\\lib\\site-packages\\py4j\\clientserver.py:506\u001b[0m, in \u001b[0;36mClientServerConnection.send_command\u001b[1;34m(self, command)\u001b[0m\n\u001b[0;32m    505\u001b[0m     logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError while sending or receiving.\u001b[39m\u001b[38;5;124m\"\u001b[39m, exc_info\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m--> 506\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JNetworkError(\n\u001b[0;32m    507\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError while sending\u001b[39m\u001b[38;5;124m\"\u001b[39m, e, proto\u001b[38;5;241m.\u001b[39mERROR_ON_SEND)\n\u001b[0;32m    509\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[1;31mPy4JNetworkError\u001b[0m: Error while sending",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mConnectionRefusedError\u001b[0m                    Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[67], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# close spark session\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[43mspark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\project\\pyspark\\venv\\lib\\site-packages\\pyspark\\sql\\session.py:1796\u001b[0m, in \u001b[0;36mSparkSession.stop\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1782\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1783\u001b[0m \u001b[38;5;124;03mStop the underlying :class:`SparkContext`.\u001b[39;00m\n\u001b[0;32m   1784\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1792\u001b[0m \u001b[38;5;124;03m>>> spark.stop()  # doctest: +SKIP\u001b[39;00m\n\u001b[0;32m   1793\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1794\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpyspark\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msql\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcontext\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SQLContext\n\u001b[1;32m-> 1796\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1797\u001b[0m \u001b[38;5;66;03m# We should clean the default session up. See SPARK-23228.\u001b[39;00m\n\u001b[0;32m   1798\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\project\\pyspark\\venv\\lib\\site-packages\\pyspark\\context.py:654\u001b[0m, in \u001b[0;36mSparkContext.stop\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    652\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_jsc\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m    653\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 654\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jsc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    655\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JError:\n\u001b[0;32m    656\u001b[0m         \u001b[38;5;66;03m# Case: SPARK-18523\u001b[39;00m\n\u001b[0;32m    657\u001b[0m         warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m    658\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnable to cleanly shutdown Spark JVM process.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    659\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m It is possible that the process has crashed,\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    660\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m been killed or may also be in a zombie state.\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    661\u001b[0m             \u001b[38;5;167;01mRuntimeWarning\u001b[39;00m,\n\u001b[0;32m    662\u001b[0m         )\n",
      "File \u001b[1;32mc:\\project\\pyspark\\venv\\lib\\site-packages\\py4j\\java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1314\u001b[0m args_command, temp_args \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_args(\u001b[38;5;241m*\u001b[39margs)\n\u001b[0;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m-> 1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend_command\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcommand\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m get_return_value(\n\u001b[0;32m   1323\u001b[0m     answer, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_id, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname)\n\u001b[0;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n",
      "File \u001b[1;32mc:\\project\\pyspark\\venv\\lib\\site-packages\\py4j\\java_gateway.py:1053\u001b[0m, in \u001b[0;36mGatewayClient.send_command\u001b[1;34m(self, command, retry, binary)\u001b[0m\n\u001b[0;32m   1051\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_should_retry(retry, connection, pne):\n\u001b[0;32m   1052\u001b[0m     logging\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mException while sending command.\u001b[39m\u001b[38;5;124m\"\u001b[39m, exc_info\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m-> 1053\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend_command\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcommand\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbinary\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbinary\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1054\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1055\u001b[0m     logging\u001b[38;5;241m.\u001b[39mexception(\n\u001b[0;32m   1056\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mException while sending command.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\project\\pyspark\\venv\\lib\\site-packages\\py4j\\java_gateway.py:1036\u001b[0m, in \u001b[0;36mGatewayClient.send_command\u001b[1;34m(self, command, retry, binary)\u001b[0m\n\u001b[0;32m   1015\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msend_command\u001b[39m(\u001b[38;5;28mself\u001b[39m, command, retry\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, binary\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m   1016\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Sends a command to the JVM. This method is not intended to be\u001b[39;00m\n\u001b[0;32m   1017\u001b[0m \u001b[38;5;124;03m       called directly by Py4J users. It is usually called by\u001b[39;00m\n\u001b[0;32m   1018\u001b[0m \u001b[38;5;124;03m       :class:`JavaMember` instances.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1034\u001b[0m \u001b[38;5;124;03m     if `binary` is `True`.\u001b[39;00m\n\u001b[0;32m   1035\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1036\u001b[0m     connection \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_connection\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1037\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1038\u001b[0m         response \u001b[38;5;241m=\u001b[39m connection\u001b[38;5;241m.\u001b[39msend_command(command)\n",
      "File \u001b[1;32mc:\\project\\pyspark\\venv\\lib\\site-packages\\py4j\\clientserver.py:284\u001b[0m, in \u001b[0;36mJavaClient._get_connection\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    281\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[0;32m    283\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m connection \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m connection\u001b[38;5;241m.\u001b[39msocket \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 284\u001b[0m     connection \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_create_new_connection\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    285\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m connection\n",
      "File \u001b[1;32mc:\\project\\pyspark\\venv\\lib\\site-packages\\py4j\\clientserver.py:291\u001b[0m, in \u001b[0;36mJavaClient._create_new_connection\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    287\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_create_new_connection\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    288\u001b[0m     connection \u001b[38;5;241m=\u001b[39m ClientServerConnection(\n\u001b[0;32m    289\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjava_parameters, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpython_parameters,\n\u001b[0;32m    290\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_property, \u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m--> 291\u001b[0m     \u001b[43mconnection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnect_to_java_server\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    292\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mset_thread_connection(connection)\n\u001b[0;32m    293\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m connection\n",
      "File \u001b[1;32mc:\\project\\pyspark\\venv\\lib\\site-packages\\py4j\\clientserver.py:438\u001b[0m, in \u001b[0;36mClientServerConnection.connect_to_java_server\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    435\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mssl_context:\n\u001b[0;32m    436\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msocket \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mssl_context\u001b[38;5;241m.\u001b[39mwrap_socket(\n\u001b[0;32m    437\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msocket, server_hostname\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjava_address)\n\u001b[1;32m--> 438\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msocket\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjava_address\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjava_port\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    439\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstream \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msocket\u001b[38;5;241m.\u001b[39mmakefile(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    440\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_connected \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[1;31mConnectionRefusedError\u001b[0m: [WinError 10061] No connection could be made because the target machine actively refused it"
     ]
    }
   ],
   "source": [
    "# close spark session\n",
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "toc-showcode": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
